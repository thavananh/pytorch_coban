{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined_cleaned_file.csv')\n",
    "df = df.drop(columns=['id'])\n",
    "df.to_csv('combined_cleaned_file.txt', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã đọc header: ['Review', 'Sentence Component', 'aspect_text', 'aspect', 'sentiment_text', 'sentiment']\n",
      "Thông tin: Không tìm thấy '\\ngiảng viên dạy' trong review (dòng 16796). Thử tìm trong sentence_component.\n",
      "Lỗi: Không tìm thấy aspect_text '\\ngiảng viên dạy' (đã làm sạch) trong cả review và sentence_component ở dòng 16796. Gán from/to = -1.\n",
      "Debug: Aspect không được thêm vào JSON (dòng 16796): {'term': '\\\\ngiảng viên dạy ', 'category': 'Teaching quality', 'polarity': 'negative', 'from': '-1', 'to': '-1'}\n",
      "--------------------\n",
      "Đã chuyển đổi thành công và lưu vào file: output_semeval_format_v3.json\n",
      "Thống kê tìm kiếm aspect_text:\n",
      "- Tìm thấy trực tiếp trong Review: 20777\n",
      "- Tìm thấy trong Sentence Component (tính được index): 0\n",
      "- Không tìm thấy / Lỗi index / Bị rỗng: 1\n",
      "- Tổng số aspect đã xử lý (dòng): 20778\n",
      "- Tổng số aspect được thêm vào JSON: 20777\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re # Import thư viện regular expression\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Hàm làm sạch text:\n",
    "    1. Thay thế các loại whitespace (newline, tab,...) bằng space thường.\n",
    "    2. Gộp nhiều space liên tiếp thành một.\n",
    "    3. Loại bỏ space thừa ở đầu/cuối.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" # Trả về chuỗi rỗng nếu không phải string\n",
    "\n",
    "    # Bước 1: Thay thế các loại whitespace bằng space thường\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "\n",
    "    # Bước 2: Thay thế nhiều khoảng trắng liên tiếp bằng một khoảng trắng duy nhất\n",
    "    # Dùng ' {2,}' để chỉ khớp với 2 hoặc nhiều dấu cách liền nhau\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "\n",
    "    # Bước 3: Loại bỏ khoảng trắng ở đầu và cuối chuỗi\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_aspect_text(text):\n",
    "    \"\"\"\n",
    "    Hàm làm sạch aspect_text: chuẩn hóa và loại bỏ dấu nháy kép bao quanh.\n",
    "    Gọi normalize_text nhiều lần để đảm bảo tính nhất quán sau các bước xử lý.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Bước 1: Chuẩn hóa text cơ bản lần đầu\n",
    "    text = normalize_text(text)\n",
    "\n",
    "    # Bước 2: Xử lý dấu nháy bao quanh (nếu có)\n",
    "    cleaned_internally = False\n",
    "    if text.startswith('\"\"\"') and text.endswith('\"\"\"'):\n",
    "        text = text[3:-3].strip() # Bỏ dấu nháy và strip khoảng trắng lộ ra\n",
    "        cleaned_internally = True\n",
    "    elif text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1].strip() # Bỏ dấu nháy và strip khoảng trắng lộ ra\n",
    "        cleaned_internally = True\n",
    "\n",
    "    # Bước 3: Nếu đã bỏ dấu nháy, chuẩn hóa lại bên trong lần nữa\n",
    "    if cleaned_internally:\n",
    "        text = normalize_text(text)\n",
    "\n",
    "    # Bước 4: Trả về kết quả đã chuẩn hóa cuối cùng\n",
    "    # (Gọi normalize_text lần cuối để đảm bảo chắc chắn, dù có thể thừa)\n",
    "    return normalize_text(text)\n",
    "\n",
    "# --- Hàm convert_to_semeval_json giữ nguyên phần còn lại ---\n",
    "# (Copy lại toàn bộ hàm convert_to_semeval_json từ phiên bản V2\n",
    "#  vì chỉ cần thay đổi 2 hàm helper normalize_text và clean_aspect_text)\n",
    "\n",
    "def convert_to_semeval_json(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Chuyển đổi file text chứa dữ liệu ABSA sang định dạng JSON giống SemEval.\n",
    "    (Phiên bản cải thiện xử lý khoảng trắng và dấu nháy - v3)\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): Đường dẫn đến file text đầu vào.\n",
    "        output_filepath (str): Đường dẫn để lưu file JSON đầu ra.\n",
    "    \"\"\"\n",
    "    sentences_data = {}\n",
    "    found_count = 0\n",
    "    not_found_count = 0\n",
    "    found_in_sc_count = 0\n",
    "\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "            header = next(f).strip().split('\\t')\n",
    "            print(f\"Đã đọc header: {header}\")\n",
    "\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                # Xử lý line đọc vào (giữ nguyên từ v2)\n",
    "                line = line.replace('\\n', ' ').replace('\\r', '')\n",
    "                parts = line.strip().split('\\t')\n",
    "\n",
    "                if len(parts) != 6:\n",
    "                    print(f\"Cảnh báo: Dòng {line_num} không có đủ 6 cột, bỏ qua: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                review_orig, sentence_component_orig, aspect_text_orig, aspect_category, sentiment_text, sentiment = parts\n",
    "\n",
    "                # --- Sử dụng hàm chuẩn hóa MỚI ---\n",
    "                review = normalize_text(review_orig)\n",
    "                cleaned_aspect_text = clean_aspect_text(aspect_text_orig)\n",
    "                sentence_component = normalize_text(sentence_component_orig)\n",
    "                # --- Hết phần sử dụng hàm chuẩn hóa MỚI ---\n",
    "\n",
    "                if not cleaned_aspect_text:\n",
    "                    # Kiểm tra aspect_text rỗng sau khi làm sạch (giữ nguyên từ v2)\n",
    "                    if aspect_text_orig.strip(): # Chỉ cảnh báo nếu gốc không phải là rỗng/khoảng trắng\n",
    "                         print(f\"Cảnh báo: aspect_text ('{aspect_text_orig}') trở thành rỗng sau khi làm sạch ở dòng {line_num}. Bỏ qua.\")\n",
    "                    # Không cần in gì nếu gốc đã là khoảng trắng\n",
    "                    continue\n",
    "\n",
    "\n",
    "                sentiment = sentiment.lower()\n",
    "                if sentiment not in [\"positive\", \"negative\", \"neutral\"]:\n",
    "                     print(f\"Cảnh báo: Sentiment không xác định '{sentiment}' ở dòng {line_num}, giữ nguyên.\")\n",
    "\n",
    "\n",
    "                start_index = -1\n",
    "                end_index = -1\n",
    "                term_to_store = aspect_text_orig # Mặc định lưu text gốc nếu không tìm thấy\n",
    "\n",
    "                # Tìm vị trí 'from' và 'to' (logic giữ nguyên từ v2)\n",
    "                start_index = review.find(cleaned_aspect_text)\n",
    "\n",
    "                if start_index != -1:\n",
    "                    end_index = start_index + len(cleaned_aspect_text)\n",
    "                    term_to_store = cleaned_aspect_text\n",
    "                    found_count += 1\n",
    "                else:\n",
    "                    # Không tìm thấy trong review, thử tìm trong sentence_component\n",
    "                    # (In thông báo tìm kiếm) - Chỉ in nếu cleaned_aspect_text có nội dung\n",
    "                    if cleaned_aspect_text:\n",
    "                        print(f\"Thông tin: Không tìm thấy '{cleaned_aspect_text}' trong review (dòng {line_num}). Thử tìm trong sentence_component.\")\n",
    "\n",
    "                    start_index_in_sc = sentence_component.find(cleaned_aspect_text)\n",
    "\n",
    "                    if start_index_in_sc != -1:\n",
    "                         if cleaned_aspect_text: # Chỉ in nếu tìm thấy text có nội dung\n",
    "                              print(f\"Thông tin: Tìm thấy '{cleaned_aspect_text}' trong sentence_component (dòng {line_num}).\")\n",
    "                         start_index_sc_in_review = review.find(sentence_component)\n",
    "\n",
    "                         if start_index_sc_in_review != -1:\n",
    "                             start_index = start_index_sc_in_review + start_index_in_sc\n",
    "                             end_index = start_index + len(cleaned_aspect_text)\n",
    "                             term_to_store = cleaned_aspect_text\n",
    "                             found_in_sc_count += 1\n",
    "                             print(f\"Thông tin: Đã tính index tương đối với review cho dòng {line_num}.\")\n",
    "                         else:\n",
    "                             if sentence_component: # Chỉ in lỗi nếu sentence_component không rỗng\n",
    "                                 print(f\"Lỗi: Tìm thấy aspect_text trong sentence_component, nhưng không tìm thấy sentence_component '{sentence_component[:50]}...' trong review ở dòng {line_num}. Gán from/to = -1.\")\n",
    "                             else: # SC rỗng thì không thể tìm thấy là đúng\n",
    "                                  print(f\"Thông tin: sentence_component rỗng ở dòng {line_num}, không thể tìm trong đó. Gán from/to = -1.\")\n",
    "                             not_found_count += 1\n",
    "                             start_index = -1\n",
    "                             end_index = -1\n",
    "                    else:\n",
    "                         # Không tìm thấy ở đâu cả\n",
    "                         if cleaned_aspect_text: # Chỉ in lỗi nếu text tìm kiếm không rỗng\n",
    "                             print(f\"Lỗi: Không tìm thấy aspect_text '{cleaned_aspect_text}' (đã làm sạch) trong cả review và sentence_component ở dòng {line_num}. Gán from/to = -1.\")\n",
    "                         # Không cần in lỗi nếu cleaned_aspect_text rỗng\n",
    "                         not_found_count += 1\n",
    "                         start_index = -1\n",
    "                         end_index = -1\n",
    "\n",
    "                # Lưu thông tin aspect (logic giữ nguyên từ v2)\n",
    "                aspect_info = {\n",
    "                    \"term\": term_to_store,\n",
    "                    \"category\": normalize_text(aspect_category),\n",
    "                    \"polarity\": sentiment,\n",
    "                    \"from\": str(start_index),\n",
    "                    \"to\": str(end_index)\n",
    "                }\n",
    "\n",
    "                # Nhóm các aspect theo câu (logic giữ nguyên từ v2)\n",
    "                review_key = review_orig.strip()\n",
    "                if review_key not in sentences_data:\n",
    "                    sentences_data[review_key] = {\n",
    "                        \"text\": review_key,\n",
    "                        \"aspects\": []\n",
    "                    }\n",
    "\n",
    "                if start_index != -1:\n",
    "                     sentences_data[review_key][\"aspects\"].append(aspect_info)\n",
    "                else:\n",
    "                     if term_to_store.strip(): # Chỉ debug nếu term gốc không phải là khoảng trắng\n",
    "                         print(f\"Debug: Aspect không được thêm vào JSON (dòng {line_num}): {aspect_info}\")\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file đầu vào tại '{input_filepath}'\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình đọc file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Chuyển đổi và ghi file JSON (logic giữ nguyên từ v2)\n",
    "    output_list = list(sentences_data.values())\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"sentences\": {\"sentence\": output_list}}, f, ensure_ascii=False, indent=4)\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Đã chuyển đổi thành công và lưu vào file: {output_filepath}\")\n",
    "        print(f\"Thống kê tìm kiếm aspect_text:\")\n",
    "        print(f\"- Tìm thấy trực tiếp trong Review: {found_count}\")\n",
    "        print(f\"- Tìm thấy trong Sentence Component (tính được index): {found_in_sc_count}\")\n",
    "        print(f\"- Không tìm thấy / Lỗi index / Bị rỗng: {not_found_count}\") # Đổi tên cho rõ hơn\n",
    "        total_processed = found_count + found_in_sc_count + not_found_count\n",
    "        print(f\"- Tổng số aspect đã xử lý (dòng): {total_processed}\") # Tổng số dòng aspect đã đọc và xử lý\n",
    "        # Tính tổng số aspect thực sự được thêm vào JSON cuối cùng\n",
    "        final_aspect_count = sum(len(s['aspects']) for s in output_list)\n",
    "        print(f\"- Tổng số aspect được thêm vào JSON: {final_aspect_count}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình ghi file JSON: {e}\")\n",
    "\n",
    "\n",
    "# --- Sử dụng hàm ---\n",
    "input_file = 'combined_cleaned_file.txt'\n",
    "# Đổi tên file output khác để không ghi đè lên file V2\n",
    "output_file = 'output_semeval_format_v3.json'\n",
    "\n",
    "if os.path.exists(input_file):\n",
    "    convert_to_semeval_json(input_file, output_file)\n",
    "else:\n",
    "    print(f\"Lỗi: File '{input_file}' không tồn tại trong thư mục hiện tại.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã đọc header: ['Review', 'Sentence Component', 'aspect_text', 'aspect', 'sentiment_text', 'sentiment']\n",
      "Đã chuyển đổi thành công và lưu 15519 dòng vào file: output_simplified_visd4sa_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def format_to_simplified_visd4sa(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Chuyển đổi file text sang định dạng JSON Lines giống UIT-ViSD4SA,\n",
    "    chỉ giữ lại text và danh sách các cặp [CATEGORY#POLARITY].\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): Đường dẫn đến file text đầu vào (tab-separated).\n",
    "        output_filepath (str): Đường dẫn để lưu file JSON Lines đầu ra.\n",
    "    \"\"\"\n",
    "    sentences_data = {} # Dictionary để nhóm các cặp Category#Polarity theo câu\n",
    "\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as infile:\n",
    "            # Bỏ qua dòng header\n",
    "            header = next(infile).strip().split('\\t')\n",
    "            print(f\"Đã đọc header: {header}\")\n",
    "\n",
    "            for line_num, line in enumerate(infile, 1):\n",
    "                parts = line.strip().split('\\t')\n",
    "\n",
    "                # Kiểm tra số lượng cột (cần ít nhất cột Review, aspect, sentiment)\n",
    "                if len(parts) != 6:\n",
    "                    print(f\"Cảnh báo: Dòng {line_num} không có đủ 6 cột, bỏ qua: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                # Lấy các cột cần thiết\n",
    "                # Column 0: Review (text gốc)\n",
    "                # Column 3: aspect (category)\n",
    "                # Column 5: sentiment (polarity)\n",
    "                review_text = parts[0].strip()\n",
    "                category = parts[3].strip()\n",
    "                polarity = parts[5].strip()\n",
    "\n",
    "                # Chuẩn hóa polarity sang uppercase giống UIT-ViSD4SA\n",
    "                # và xử lý các trường hợp có thể có\n",
    "                if polarity.lower() == 'positive':\n",
    "                    polarity_formatted = 'POSITIVE'\n",
    "                elif polarity.lower() == 'negative':\n",
    "                    polarity_formatted = 'NEGATIVE'\n",
    "                elif polarity.lower() == 'neutral':\n",
    "                    polarity_formatted = 'NEUTRAL'\n",
    "                else:\n",
    "                    print(f\"Cảnh báo: Polarity không xác định '{polarity}' ở dòng {line_num}. Sử dụng giá trị gốc.\")\n",
    "                    # Quyết định cách xử lý: bỏ qua, dùng giá trị gốc, hoặc gán mặc định\n",
    "                    # Ở đây ta dùng giá trị gốc và viết hoa\n",
    "                    polarity_formatted = polarity.upper()\n",
    "\n",
    "                # Tạo chuỗi \"CATEGORY#POLARITY\"\n",
    "                label_string = f\"{category}#{polarity_formatted}\"\n",
    "\n",
    "                # Thêm vào dictionary, sử dụng set để tránh trùng lặp nhãn cho cùng một câu\n",
    "                if review_text not in sentences_data:\n",
    "                    sentences_data[review_text] = set() # Dùng set để tự động loại bỏ trùng lặp\n",
    "                sentences_data[review_text].add(label_string)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file đầu vào tại '{input_filepath}'\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình đọc file input: {e}\")\n",
    "        return\n",
    "\n",
    "    # Ghi ra file JSON Lines\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "            count = 0\n",
    "            for text, labels_set in sentences_data.items():\n",
    "                # Tạo đối tượng dictionary cho dòng JSON\n",
    "                json_object = {\n",
    "                    \"text\": text,\n",
    "                    \"labels\": sorted(list(labels_set)) # Chuyển set thành list, sắp xếp để nhất quán (tùy chọn)\n",
    "                }\n",
    "                # Chuyển dictionary thành chuỗi JSON\n",
    "                json_line = json.dumps(json_object, ensure_ascii=False)\n",
    "                # Ghi chuỗi JSON vào file, theo sau là ký tự xuống dòng\n",
    "                outfile.write(json_line + '\\n')\n",
    "                count += 1\n",
    "        print(f\"Đã chuyển đổi thành công và lưu {count} dòng vào file: {output_filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình ghi file JSON Lines: {e}\")\n",
    "\n",
    "# --- Sử dụng hàm ---\n",
    "input_file = 'combined_cleaned_file.txt' # File input của bạn\n",
    "output_file = 'output_simplified_visd4sa_format.jsonl' # File output định dạng JSON Lines (.jsonl)\n",
    "\n",
    "# Kiểm tra xem file input có tồn tại không\n",
    "if os.path.exists(input_file):\n",
    "    format_to_simplified_visd4sa(input_file, output_file)\n",
    "else:\n",
    "    print(f\"Lỗi: File '{input_file}' không tồn tại trong thư mục hiện tại.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 14545\n",
      "Test: 4156\n",
      "Validation: 2078\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "df = pd.read_csv('combined_cleaned_file.csv')\n",
    "\n",
    "# Ánh xạ aspect -> số nguyên\n",
    "aspect_mapping = {\n",
    "    \"Course information\": 0,\n",
    "    \"General review\": 1,\n",
    "    \"Learning environment\": 2,\n",
    "    \"Organization and management\": 3,\n",
    "    \"Support from lecturers\": 4,\n",
    "    \"Teaching quality\": 5,\n",
    "    \"Test and evaluation\": 6,\n",
    "    \"Workload\": 7\n",
    "}\n",
    "\n",
    "# Ánh xạ sentiment -> số nguyên\n",
    "sentiment_mapping = {\n",
    "    \"Negative\": 0,\n",
    "    \"Neutral\": 1,\n",
    "    \"Positive\": 2\n",
    "}\n",
    "\n",
    "# Thay thế aspect và sentiment bằng số tương ứng\n",
    "df[\"aspect_encoded\"] = df[\"aspect\"].map(aspect_mapping)\n",
    "df[\"sentiment_encoded\"] = df[\"sentiment\"].map(sentiment_mapping)\n",
    "\n",
    "df = df.drop(columns=['id'])\n",
    "df = df.rename({'Sentence Component': \"text\"})\n",
    "# Chia lần 1: train (70%) và temp (30%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Chia lần 2: temp thành test (20%) và val (10%)\n",
    "test_df, val_df = train_test_split(temp_df, test_size=1/3, random_state=42)  # 1/3 của 30% là 10%\n",
    "\n",
    "# Kiểm tra kích thước\n",
    "print(f'Train: {len(train_df)}')\n",
    "print(f'Test: {len(test_df)}')\n",
    "print(f'Validation: {len(val_df)}')\n",
    "\n",
    "# Lưu lại nếu cần\n",
    "train_df.to_csv('uit_train_absa.csv', index=False)\n",
    "test_df.to_csv('uit_test_absa.csv', index=False)\n",
    "val_df.to_csv('uit_val_absa.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã đọc header: ['Review', 'Sentence Component', 'aspect_text', 'aspect', 'sentiment_text', 'sentiment']\n",
      "--------------------\n",
      "Đã chuyển đổi thành công sang định dạng ASCA và lưu vào file: output_asca_format.json\n",
      "Tổng số câu được xử lý: 15519\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict # Sử dụng defaultdict để nhóm dễ dàng hơn\n",
    "\n",
    "def format_for_asca_json(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Chuyển đổi file text sang định dạng JSON cho tác vụ ASCA.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): Đường dẫn đến file text đầu vào (tab-separated).\n",
    "        output_filepath (str): Đường dẫn để lưu file JSON đầu ra.\n",
    "    \"\"\"\n",
    "    # Sử dụng defaultdict(set) để tự động xử lý câu mới và tránh trùng lặp cặp (category, polarity)\n",
    "    sentences_data = defaultdict(set)\n",
    "\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as infile:\n",
    "            header = next(infile).strip().split('\\t')\n",
    "            print(f\"Đã đọc header: {header}\")\n",
    "\n",
    "            for line_num, line in enumerate(infile, 1):\n",
    "                # Loại bỏ ký tự xuống dòng trong chính dòng đọc vào\n",
    "                line = line.replace('\\n', ' ').replace('\\r', '')\n",
    "                parts = line.strip().split('\\t')\n",
    "\n",
    "                if len(parts) != 6:\n",
    "                    print(f\"Cảnh báo: Dòng {line_num} không có đủ 6 cột, bỏ qua: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                # Lấy các cột cần thiết\n",
    "                review_text = parts[0].strip()\n",
    "                category = parts[3].strip()\n",
    "                polarity = parts[5].strip().lower() # Lấy polarity và chuẩn hóa lowercase\n",
    "\n",
    "                # Kiểm tra giá trị polarity chuẩn\n",
    "                if polarity not in [\"positive\", \"negative\", \"neutral\"]:\n",
    "                    print(f\"Cảnh báo: Polarity không chuẩn '{polarity}' ở dòng {line_num}. Sử dụng giá trị gốc (viết thường).\")\n",
    "                    # Bạn có thể quyết định xử lý khác ở đây nếu muốn\n",
    "                    # polarity = \"neutral\" # Ví dụ: Gán mặc định là neutral\n",
    "\n",
    "                # Thêm cặp (category, polarity) vào set của câu tương ứng\n",
    "                # Dùng tuple vì set yêu cầu phần tử hashable\n",
    "                if review_text and category: # Chỉ thêm nếu review và category không rỗng\n",
    "                    sentences_data[review_text].add((category, polarity))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file đầu vào tại '{input_filepath}'\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình đọc file input: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Chuyển đổi dữ liệu đã nhóm thành định dạng list JSON mong muốn\n",
    "    output_list = []\n",
    "    for text, label_tuples in sentences_data.items():\n",
    "        # Chuyển set các tuple thành list các dictionary\n",
    "        aspect_categories_list = [\n",
    "            {\"category\": cat, \"polarity\": pol}\n",
    "            for cat, pol in sorted(list(label_tuples)) # Sắp xếp để thứ tự nhất quán (tùy chọn)\n",
    "        ]\n",
    "\n",
    "        # Tạo đối tượng sentence\n",
    "        sentence_object = {\n",
    "            \"text\": text,\n",
    "            \"aspectCategories\": aspect_categories_list\n",
    "        }\n",
    "        output_list.append(sentence_object)\n",
    "\n",
    "    # Tạo cấu trúc JSON cuối cùng (ví dụ: đặt trong key \"sentences\")\n",
    "    final_json_structure = {\"sentences\": output_list}\n",
    "\n",
    "    # Ghi ra file JSON\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(final_json_structure, outfile, ensure_ascii=False, indent=4)\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Đã chuyển đổi thành công sang định dạng ASCA và lưu vào file: {output_filepath}\")\n",
    "        print(f\"Tổng số câu được xử lý: {len(output_list)}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình ghi file JSON: {e}\")\n",
    "\n",
    "# --- Sử dụng hàm ---\n",
    "input_file = 'combined_cleaned_file.txt' # File input của bạn\n",
    "output_file = 'output_asca_format.json' # File output định dạng JSON cho ASCA\n",
    "\n",
    "if os.path.exists(input_file):\n",
    "    format_for_asca_json(input_file, output_file)\n",
    "else:\n",
    "    print(f\"Lỗi: File '{input_file}' không tồn tại trong thư mục hiện tại.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
