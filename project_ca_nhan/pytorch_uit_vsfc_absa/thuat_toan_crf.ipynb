{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã đọc 15519 câu từ file JSON gốc.\n",
      "--------------------\n",
      "Thống kê quá trình chuẩn bị dữ liệu (Data Duplication):\n",
      "- Số câu có aspect chồng lấn được nhân bản (ước lượng): 1336\n",
      "- Số lần không khớp span ký tự với span token: 9\n",
      "--------------------\n",
      "Đã chuẩn bị xong 17786 mẫu dữ liệu (sau khi nhân bản).\n",
      "Kích thước tập huấn luyện: 14228\n",
      "Kích thước tập kiểm thử: 3558\n",
      "Bắt đầu huấn luyện mô hình CRF...\n",
      "Huấn luyện hoàn tất.\n",
      "Bắt đầu dự đoán trên tập kiểm thử...\n",
      "\n",
      "--- Kết quả đánh giá theo Category (Span-based) ---\n",
      "Aspect Category                Precision    Recall       F1-score    \n",
      "--------------------------------------------------------------------\n",
      "Course information             36.232%      24.752%      29.412%     \n",
      "General review                 36.774%      19.128%      25.166%     \n",
      "Learning environment           50.327%      34.222%      40.741%     \n",
      "Organization and management    23.626%      14.879%      18.259%     \n",
      "Support from lecturers         27.815%      17.612%      21.568%     \n",
      "Teaching quality               45.756%      38.988%      42.102%     \n",
      "Test and evaluation            34.247%      18.519%      24.038%     \n",
      "Workload                       37.097%      27.219%      31.399%     \n",
      "--------------------------------------------------------------------\n",
      "Macro Average                  36.484%      24.415%      29.085%     \n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from underthesea import word_tokenize\n",
    "import sklearn_crfsuite\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Bỏ import metrics cũ của sklearn_crfsuite nếu không dùng nữa\n",
    "# from sklearn_crfsuite import metrics\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import re # Thêm re để dùng trong get_category_from_tag nếu cần\n",
    "\n",
    "# --- Hàm load_data, word2features, prepare_data_with_duplication giữ nguyên ---\n",
    "# (Copy lại các hàm này từ phiên bản tốt nhất trước đó của bạn)\n",
    "def load_data(filepath):\n",
    "    # (Giữ nguyên code hàm load_data)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            if \"sentences\" in data and \"sentence\" in data[\"sentences\"]:\n",
    "                 return data[\"sentences\"][\"sentence\"]\n",
    "            else:\n",
    "                 print(\"Lỗi: Cấu trúc JSON không đúng, không tìm thấy 'sentences.sentence'\")\n",
    "                 if isinstance(data, list):\n",
    "                      print(\"Thử giả định file JSON là một danh sách các câu...\")\n",
    "                      return data\n",
    "                 return []\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file {filepath}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Lỗi: File {filepath} không phải là định dạng JSON hợp lệ.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định khi đọc file: {e}\")\n",
    "        return []\n",
    "\n",
    "def word2features(sent_tokens, i):\n",
    "    # (Giữ nguyên code hàm word2features)\n",
    "    word = sent_tokens[i]\n",
    "    features = {\n",
    "        'bias': 1.0, 'word.lower()': word.lower(), 'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(), 'word.isdigit()': word.isdigit(),\n",
    "        'word.suffix(3)': word[-3:], 'word.prefix(3)': word[:3],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent_tokens[i-1]\n",
    "        features.update({'-1:word.lower()': word1.lower(), '-1:word.istitle()': word1.istitle(), '-1:word.isupper()': word1.isupper()})\n",
    "    else: features['BOS'] = True\n",
    "    if i < len(sent_tokens)-1:\n",
    "        word1 = sent_tokens[i+1]\n",
    "        features.update({'+1:word.lower()': word1.lower(), '+1:word.istitle()': word1.istitle(), '+1:word.isupper()': word1.isupper()})\n",
    "    else: features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "def prepare_data_with_duplication(sentences_data):\n",
    "    # (Giữ nguyên code hàm prepare_data_with_duplication)\n",
    "    final_data = [] # List cuối cùng chứa các cặp (features, labels)\n",
    "    span_mapping_fail_count = 0\n",
    "    sentences_with_overlap = 0\n",
    "\n",
    "    for sentence_idx, sentence_info in enumerate(sentences_data):\n",
    "        text = sentence_info['text']\n",
    "        aspects_in = sentence_info.get('aspects', [])\n",
    "\n",
    "        # Bước 1: Tokenize và tính toán character spans\n",
    "        tokens = word_tokenize(text)\n",
    "        if not tokens: continue\n",
    "        token_spans = []\n",
    "        current_pos = 0\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                start = text.index(token, current_pos)\n",
    "                end = start + len(token)\n",
    "                token_spans.append({'text': token, 'start': start, 'end': end})\n",
    "                current_pos = end\n",
    "            except ValueError:\n",
    "                # print(f\"Cảnh báo: Không thể khớp token '{token}' lại với text gốc trong câu {sentence_idx}. Gán span không hợp lệ.\") # Giảm bớt log\n",
    "                token_spans.append({'text': token, 'start': -1, 'end': -1})\n",
    "                current_pos += len(token)\n",
    "\n",
    "        # Bước 2: Map aspects to token indices và xác định chồng lấn\n",
    "        aspect_token_spans = []\n",
    "        token_to_aspect_indices = defaultdict(list)\n",
    "        valid_aspects = []\n",
    "\n",
    "        for aspect_idx, aspect_data in enumerate(aspects_in):\n",
    "            try:\n",
    "                from_char = int(aspect_data['from'])\n",
    "                to_char = int(aspect_data['to'])\n",
    "                category = aspect_data.get('category')\n",
    "                term = aspect_data.get('term')\n",
    "\n",
    "                if category and from_char != -1 and to_char != -1 and from_char < to_char:\n",
    "                    start_token_idx, end_token_idx = -1, -1\n",
    "                    for i, span_info in enumerate(token_spans):\n",
    "                        if span_info['start'] == -1: continue\n",
    "                        tok_start, tok_end = span_info['start'], span_info['end']\n",
    "                        if start_token_idx == -1 and tok_start <= from_char < tok_end: start_token_idx = i\n",
    "                        if tok_start < to_char and tok_start <= (to_char - 1) < tok_end: end_token_idx = i\n",
    "\n",
    "                    if start_token_idx != -1 and end_token_idx != -1 and start_token_idx <= end_token_idx:\n",
    "                        current_aspect_info = {'id': aspect_idx,'category': category,'term': term,'start_token': start_token_idx,'end_token': end_token_idx,'involved_in_overlap': False}\n",
    "                        valid_aspects.append(current_aspect_info)\n",
    "                        for i in range(start_token_idx, end_token_idx + 1):\n",
    "                            if i < len(tokens): token_to_aspect_indices[i].append(len(valid_aspects) - 1)\n",
    "                    else:\n",
    "                        span_mapping_fail_count += 1\n",
    "                        # print(f\"Cảnh báo: Không thể khớp span ký tự [{from_char}-{to_char}] (term '{term}') với span token câu {sentence_idx}.\") # Giảm bớt log\n",
    "\n",
    "            except (ValueError, TypeError, KeyError): continue\n",
    "\n",
    "        overlapping_aspect_indices = set()\n",
    "        for token_idx, mapped_aspect_idxs in token_to_aspect_indices.items():\n",
    "            if len(mapped_aspect_idxs) > 1:\n",
    "                for aspect_v_idx in mapped_aspect_idxs:\n",
    "                    if aspect_v_idx < len(valid_aspects): # Check index validity\n",
    "                         overlapping_aspect_indices.add(aspect_v_idx)\n",
    "                         valid_aspects[aspect_v_idx]['involved_in_overlap'] = True\n",
    "\n",
    "        non_overlapping_aspects = [a for idx, a in enumerate(valid_aspects) if idx not in overlapping_aspect_indices]\n",
    "        overlapping_aspects = [a for idx, a in enumerate(valid_aspects) if idx in overlapping_aspect_indices]\n",
    "\n",
    "        # Bước 3: Tạo features\n",
    "        sentence_features = [word2features(tokens, i) for i in range(len(tokens))]\n",
    "\n",
    "        # Bước 4: Tạo các mẫu huấn luyện (features, labels)\n",
    "        generated_samples = []\n",
    "        if not overlapping_aspects:\n",
    "            labels = ['O'] * len(tokens)\n",
    "            processed_non_overlap = set()\n",
    "            for aspect in non_overlapping_aspects:\n",
    "                start_tok, end_tok = aspect['start_token'], aspect['end_token']\n",
    "                category = aspect['category']\n",
    "                can_add = True\n",
    "                for i in range(start_tok, end_tok + 1):\n",
    "                    if i in processed_non_overlap: can_add = False; break\n",
    "                if can_add:\n",
    "                    try:\n",
    "                        labels[start_tok] = f'B-{category}'\n",
    "                        processed_non_overlap.add(start_tok)\n",
    "                        for i in range(start_tok + 1, end_tok + 1):\n",
    "                             if i < len(labels): labels[i] = f'I-{category}'; processed_non_overlap.add(i)\n",
    "                    except IndexError: pass\n",
    "            generated_samples.append((sentence_features, labels))\n",
    "        else:\n",
    "            sentences_with_overlap += 1\n",
    "            base_labels = ['O'] * len(tokens)\n",
    "            processed_non_overlap = set()\n",
    "            for aspect in non_overlapping_aspects:\n",
    "                start_tok, end_tok = aspect['start_token'], aspect['end_token']\n",
    "                category = aspect['category']\n",
    "                can_add_base = True\n",
    "                for i in range(start_tok, end_tok + 1):\n",
    "                     if i in processed_non_overlap: can_add_base = False; break\n",
    "                if can_add_base:\n",
    "                    try:\n",
    "                        base_labels[start_tok] = f'B-{category}'\n",
    "                        processed_non_overlap.add(start_tok)\n",
    "                        for i in range(start_tok + 1, end_tok + 1):\n",
    "                             if i < len(base_labels): base_labels[i] = f'I-{category}'; processed_non_overlap.add(i)\n",
    "                    except IndexError: pass\n",
    "\n",
    "            for target_aspect in overlapping_aspects:\n",
    "                labels_for_target = base_labels[:]\n",
    "                start_tok, end_tok = target_aspect['start_token'], target_aspect['end_token']\n",
    "                category = target_aspect['category']\n",
    "                try:\n",
    "                    labels_for_target[start_tok] = f'B-{category}'\n",
    "                    for i in range(start_tok + 1, end_tok + 1):\n",
    "                        if i < len(labels_for_target): labels_for_target[i] = f'I-{category}'\n",
    "                    generated_samples.append((sentence_features, labels_for_target))\n",
    "                except IndexError: pass # Bỏ qua nếu lỗi index\n",
    "\n",
    "        final_data.extend(generated_samples)\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Thống kê quá trình chuẩn bị dữ liệu (Data Duplication):\")\n",
    "    print(f\"- Số câu có aspect chồng lấn được nhân bản (ước lượng): {sentences_with_overlap}\") # Đây là số câu gốc có chồng lấn\n",
    "    print(f\"- Số lần không khớp span ký tự với span token: {span_mapping_fail_count}\")\n",
    "    print(\"-\" * 20)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "# --- Các hàm mới cho việc đánh giá theo Category ---\n",
    "\n",
    "def get_category_from_tag(tag):\n",
    "    \"\"\"Trích xuất category từ nhãn BIO (ví dụ: 'B-Teaching quality' -> 'Teaching quality').\"\"\"\n",
    "    if tag is None or tag == 'O':\n",
    "        return None\n",
    "    # Sử dụng regex để tìm category sau tiền tố B- hoặc I-\n",
    "    match = re.match(r'^[BI]-(.*)$', tag)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    # print(f\"Cảnh báo: Không thể trích xuất category từ tag '{tag}'\") # Bỏ comment nếu muốn debug tag lạ\n",
    "    return None # Hoặc trả về tag gốc nếu không có tiền tố B/I?\n",
    "\n",
    "def bio_tags_to_spans(tags):\n",
    "    \"\"\"\n",
    "    Chuyển đổi một chuỗi nhãn BIO thành một set các cụm (category, start, end).\n",
    "    Ví dụ: ['O', 'B-CAT1', 'I-CAT1', 'O', 'B-CAT2'] -> {('CAT1', 1, 2), ('CAT2', 4, 4)}\n",
    "    \"\"\"\n",
    "    spans = set()\n",
    "    current_category = None\n",
    "    start_index = -1\n",
    "\n",
    "    for i, tag in enumerate(tags):\n",
    "        category = get_category_from_tag(tag) # Lấy category (hoặc None)\n",
    "\n",
    "        if tag.startswith('B-'):\n",
    "            # Nếu đang có span cũ, lưu lại\n",
    "            if current_category is not None:\n",
    "                spans.add((current_category, start_index, i - 1))\n",
    "            # Bắt đầu span mới\n",
    "            current_category = category\n",
    "            start_index = i\n",
    "        elif tag.startswith('I-'):\n",
    "            # Nếu không có span trước đó HOẶC category thay đổi -> lỗi I không hợp lệ\n",
    "            if current_category is None or category != current_category:\n",
    "                # Đang có I mà không có B hoặc khác category -> kết thúc span cũ (nếu có) và coi I này như O\n",
    "                if current_category is not None:\n",
    "                    spans.add((current_category, start_index, i - 1))\n",
    "                current_category = None\n",
    "                start_index = -1\n",
    "            # Else: Vẫn tiếp tục span hiện tại, không cần làm gì (chỉ cập nhật end khi gặp B hoặc O hoặc hết chuỗi)\n",
    "        else: # Tag là 'O' hoặc không hợp lệ (None)\n",
    "            # Nếu đang có span, kết thúc và lưu lại\n",
    "            if current_category is not None:\n",
    "                spans.add((current_category, start_index, i - 1))\n",
    "            # Reset\n",
    "            current_category = None\n",
    "            start_index = -1\n",
    "\n",
    "    # Lưu lại span cuối cùng nếu có\n",
    "    if current_category is not None:\n",
    "        spans.add((current_category, start_index, len(tags) - 1))\n",
    "\n",
    "    return spans\n",
    "\n",
    "def calculate_category_metrics(y_true_bio, y_pred_bio):\n",
    "    \"\"\"\n",
    "    Tính toán Precision, Recall, F1 cho từng category dựa trên so khớp span.\n",
    "\n",
    "    Args:\n",
    "        y_true_bio: List các list nhãn BIO thực tế.\n",
    "        y_pred_bio: List các list nhãn BIO dự đoán.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary chứa P, R, F1 cho từng category.\n",
    "              Ví dụ: {'CAT1': {'P': ..., 'R': ..., 'F1': ...}, ...}\n",
    "    \"\"\"\n",
    "    true_positives = defaultdict(int)\n",
    "    false_positives = defaultdict(int)\n",
    "    false_negatives = defaultdict(int)\n",
    "    all_categories = set()\n",
    "\n",
    "    if len(y_true_bio) != len(y_pred_bio):\n",
    "        raise ValueError(\"Số lượng câu trong y_true và y_pred không khớp!\")\n",
    "\n",
    "    for i in range(len(y_true_bio)):\n",
    "        true_tags = y_true_bio[i]\n",
    "        pred_tags = y_pred_bio[i]\n",
    "\n",
    "        true_spans = bio_tags_to_spans(true_tags)\n",
    "        pred_spans = bio_tags_to_spans(pred_tags)\n",
    "\n",
    "        # Cập nhật danh sách các category gặp phải\n",
    "        for category, _, _ in true_spans: all_categories.add(category)\n",
    "        for category, _, _ in pred_spans: all_categories.add(category)\n",
    "\n",
    "        # Tính TP và FP (duyệt qua các span dự đoán)\n",
    "        for pred_span in pred_spans:\n",
    "            category = pred_span[0]\n",
    "            if pred_span in true_spans:\n",
    "                true_positives[category] += 1\n",
    "            else:\n",
    "                false_positives[category] += 1\n",
    "\n",
    "        # Tính FN (duyệt qua các span thực tế)\n",
    "        for true_span in true_spans:\n",
    "            category = true_span[0]\n",
    "            if true_span not in pred_spans:\n",
    "                false_negatives[category] += 1\n",
    "\n",
    "    # Tính toán P, R, F1 cho từng category\n",
    "    results = {}\n",
    "    sorted_categories = sorted(list(all_categories)) # Sắp xếp alphabet\n",
    "\n",
    "    for category in sorted_categories:\n",
    "        tp = true_positives[category]\n",
    "        fp = false_positives[category]\n",
    "        fn = false_negatives[category]\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        results[category] = {\n",
    "            'P': precision,\n",
    "            'R': recall,\n",
    "            'F1': f1,\n",
    "            # 'TP': tp, 'FP': fp, 'FN': fn # Bỏ comment nếu muốn xem cả số lượng\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- 4. Main Script (Phần đánh giá được cập nhật) ---\n",
    "input_json_file = 'output_semeval_format_v3.json' # Hoặc file JSON bạn muốn dùng\n",
    "raw_sentences = load_data(input_json_file)\n",
    "\n",
    "if not raw_sentences:\n",
    "    print(\"Không có dữ liệu để xử lý.\")\n",
    "else:\n",
    "    print(f\"Đã đọc {len(raw_sentences)} câu từ file JSON gốc.\")\n",
    "\n",
    "    prepared_data = prepare_data_with_duplication(raw_sentences)\n",
    "    if not prepared_data:\n",
    "         print(\"Không thể chuẩn bị dữ liệu từ các câu đã đọc.\")\n",
    "    else:\n",
    "        X = [item[0] for item in prepared_data]\n",
    "        y = [item[1] for item in prepared_data]\n",
    "        print(f\"Đã chuẩn bị xong {len(X)} mẫu dữ liệu (sau khi nhân bản).\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        print(f\"Kích thước tập huấn luyện: {len(X_train)}\")\n",
    "        print(f\"Kích thước tập kiểm thử: {len(X_test)}\")\n",
    "\n",
    "        crf = sklearn_crfsuite.CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "\n",
    "        print(\"Bắt đầu huấn luyện mô hình CRF...\")\n",
    "        try:\n",
    "            crf.fit(X_train, y_train)\n",
    "            print(\"Huấn luyện hoàn tất.\")\n",
    "\n",
    "            print(\"Bắt đầu dự đoán trên tập kiểm thử...\")\n",
    "            y_pred = crf.predict(X_test) # Đây là list các list nhãn BIO dự đoán\n",
    "\n",
    "            # --- Đánh giá theo Category ---\n",
    "            print(\"\\n--- Kết quả đánh giá theo Category (Span-based) ---\")\n",
    "            category_results = calculate_category_metrics(y_test, y_pred) # y_test cũng là list các list nhãn BIO\n",
    "\n",
    "            # In kết quả dạng bảng giống hình ảnh\n",
    "            print(f\"{'Aspect Category':<30} {'Precision':<12} {'Recall':<12} {'F1-score':<12}\")\n",
    "            print(\"-\" * 68)\n",
    "\n",
    "            all_precisions = []\n",
    "            all_recalls = []\n",
    "            all_f1s = []\n",
    "\n",
    "            for category, metrics_dict in category_results.items():\n",
    "                p = metrics_dict['P']\n",
    "                r = metrics_dict['R']\n",
    "                f1 = metrics_dict['F1']\n",
    "                all_precisions.append(p)\n",
    "                all_recalls.append(r)\n",
    "                all_f1s.append(f1)\n",
    "                print(f\"{category:<30} {p:<12.3%} {r:<12.3%} {f1:<12.3%}\") # Format phần trăm\n",
    "\n",
    "            # --- (Tùy chọn) Tính trung bình Macro ---\n",
    "            if all_f1s: # Tránh chia cho 0 nếu không có category nào\n",
    "                 macro_p = sum(all_precisions) / len(all_precisions)\n",
    "                 macro_r = sum(all_recalls) / len(all_recalls)\n",
    "                 macro_f1 = sum(all_f1s) / len(all_f1s)\n",
    "                 # Hoặc tính F1 từ P, R trung bình:\n",
    "                 # macro_f1_alt = 2 * (macro_p * macro_r) / (macro_p + macro_r) if (macro_p + macro_r) > 0 else 0.0\n",
    "                 print(\"-\" * 68)\n",
    "                 print(f\"{'Macro Average':<30} {macro_p:<12.3%} {macro_r:<12.3%} {macro_f1:<12.3%}\")\n",
    "            print(\"-\" * 68)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Đã xảy ra lỗi trong quá trình huấn luyện hoặc dự đoán/đánh giá: {e}\")\n",
    "             import traceback\n",
    "             traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def convert_to_semeval_json(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Chuyển đổi file text chứa dữ liệu ABSA sang định dạng JSON giống SemEval.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): Đường dẫn đến file text đầu vào.\n",
    "        output_filepath (str): Đường dẫn để lưu file JSON đầu ra.\n",
    "    \"\"\"\n",
    "    sentences_data = {}\n",
    "\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "            # Bỏ qua dòng header\n",
    "            header = next(f).strip().split('\\t')\n",
    "            print(f\"Đã đọc header: {header}\") # In header để kiểm tra\n",
    "\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                parts = line.strip().split('\\t')\n",
    "\n",
    "                # Kiểm tra số lượng cột có đủ không\n",
    "                if len(parts) != 6:\n",
    "                    print(f\"Cảnh báo: Dòng {line_num} không có đủ 6 cột, bỏ qua: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                review, sentence_component, aspect_text, aspect_category, sentiment_text, sentiment = parts\n",
    "\n",
    "                # Chuẩn hóa sentiment (ví dụ: Positive -> positive)\n",
    "                # Nếu sentiment của bạn đã chuẩn rồi thì có thể bỏ qua bước này\n",
    "                sentiment = sentiment.lower()\n",
    "                if sentiment not in [\"positive\", \"negative\", \"neutral\"]:\n",
    "                     # Hoặc xử lý theo cách khác nếu có các giá trị khác\n",
    "                     print(f\"Cảnh báo: Sentiment không xác định '{sentiment}' ở dòng {line_num}, giữ nguyên.\")\n",
    "\n",
    "\n",
    "                # Tìm vị trí 'from' và 'to' của aspect_text trong review\n",
    "                # Sử dụng review gốc làm text gốc để tìm index\n",
    "                start_index = review.find(aspect_text)\n",
    "\n",
    "                if start_index == -1:\n",
    "                    # Thử tìm trong sentence_component nếu không thấy trong review\n",
    "                    # Điều này ít khả năng xảy ra nếu cấu trúc file đúng\n",
    "                    print(f\"Cảnh báo: Không tìm thấy aspect_text '{aspect_text}' trong review ở dòng {line_num}. Thử tìm trong sentence_component.\")\n",
    "                    start_index = sentence_component.find(aspect_text)\n",
    "                    if start_index == -1:\n",
    "                         print(f\"Lỗi: Không tìm thấy aspect_text '{aspect_text}' trong cả review và sentence_component ở dòng {line_num}. Gán from/to = -1.\")\n",
    "                         end_index = -1\n",
    "                    else:\n",
    "                         # Nếu tìm thấy trong sentence_component, cần điều chỉnh index dựa trên vị trí của sentence_component trong review\n",
    "                         # Tuy nhiên, logic này phức tạp và file có vẻ dùng review làm gốc.\n",
    "                         # Tạm thời vẫn tính 'to' dựa trên len(aspect_text)\n",
    "                         end_index = start_index + len(aspect_text)\n",
    "                         print(f\"Cảnh báo: aspect_text tìm thấy trong sentence_component, index có thể không chính xác với review gốc.\")\n",
    "\n",
    "                else:\n",
    "                    end_index = start_index + len(aspect_text)\n",
    "\n",
    "                # Lưu thông tin aspect\n",
    "                aspect_info = {\n",
    "                    # Theo cấu trúc SemEval thường dùng 'term' cho text span\n",
    "                    \"term\": aspect_text,\n",
    "                    \"category\": aspect_category,\n",
    "                    \"polarity\": sentiment,\n",
    "                    \"from\": str(start_index), # SemEval thường lưu index dạng string\n",
    "                    \"to\": str(end_index)      # SemEval thường lưu index dạng string\n",
    "                }\n",
    "\n",
    "                # Nhóm các aspect theo câu (review)\n",
    "                if review not in sentences_data:\n",
    "                    sentences_data[review] = {\n",
    "                        \"text\": review,\n",
    "                        # SemEval thường có cả 'aspectTerms' và 'aspectCategories'\n",
    "                        # Ở đây ta gộp thông tin vào một list 'aspects' cho tiện\n",
    "                        # vì mỗi entry đã có cả category và term (aspect_text)\n",
    "                        \"aspects\": []\n",
    "                    }\n",
    "                sentences_data[review][\"aspects\"].append(aspect_info)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file đầu vào tại '{input_filepath}'\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình đọc file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Chuyển đổi dictionary thành list theo cấu trúc mong muốn\n",
    "    output_list = list(sentences_data.values())\n",
    "\n",
    "    # Ghi ra file JSON\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            # ensure_ascii=False để giữ nguyên ký tự tiếng Việt\n",
    "            # indent=4 để file JSON dễ đọc hơn\n",
    "            json.dump({\"sentences\": {\"sentence\": output_list}}, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Đã chuyển đổi thành công và lưu vào file: {output_filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình ghi file JSON: {e}\")\n",
    "\n",
    "# --- Sử dụng hàm ---\n",
    "input_file = 'combined_cleaned_file.txt' # Tên file bạn đã tải lên\n",
    "output_file = 'output_semeval_format.json'\n",
    "\n",
    "# Kiểm tra xem file input có tồn tại không\n",
    "if os.path.exists(input_file):\n",
    "    convert_to_semeval_json(input_file, output_file)\n",
    "else:\n",
    "    print(f\"Lỗi: File '{input_file}' không tồn tại trong thư mục hiện tại.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
