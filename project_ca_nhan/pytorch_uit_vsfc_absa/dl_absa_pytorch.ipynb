{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Building vocabulary and tag set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 345\u001b[39m\n\u001b[32m    342\u001b[39m raw_data = load_data(DATA_FILE)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_data: exit()\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m word2idx, idx2word, tag2idx, idx2tag = \u001b[43mbuild_vocab_and_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m sentences_idx, tags_idx = preprocess_data(raw_data, word2idx, tag2idx, MAX_LEN)\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# 2. Create datasets and dataloaders\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mbuild_vocab_and_tags\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     59\u001b[39m text = sentence_data[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     60\u001b[39m aspects = sentence_data[\u001b[33m'\u001b[39m\u001b[33maspects\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m words.update(tokens)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Generate unified tags temporarily just to count them\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(sentence):\n\u001b[32m     47\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Tokenizes a Vietnamese sentence.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munderthesea\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/Ai_ENV/lib/python3.11/site-packages/underthesea/pipeline/word_tokenize/__init__.py:48\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(sentence, format, use_token_normalize, fixed_words)\u001b[39m\n\u001b[32m     46\u001b[39m output = []\n\u001b[32m     47\u001b[39m num_words = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tag, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tag == \u001b[33m\"\u001b[39m\u001b[33mI-W\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_words > \u001b[32m0\u001b[39m:\n\u001b[32m     50\u001b[39m         output[-\u001b[32m1\u001b[39m] = output[-\u001b[32m1\u001b[39m] + \u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + token\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import underthesea # For Vietnamese word tokenization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = 'output_semeval_format_v3_no_overlap.txt'\n",
    "MAX_LEN = 100       # Maximum sequence length\n",
    "EMBEDDING_DIM = 100 # Dimension of word embeddings\n",
    "LSTM_HIDDEN_DIM = 128 # Dimension of LSTM hidden states (per direction)\n",
    "CNN_FILTERS = 50    # Number of filters for each kernel size in CNN\n",
    "CNN_KERNEL_SIZES = [2, 3, 4] # Kernel sizes for CNN\n",
    "DROPOUT_RATE = 0.5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10         # Adjust as needed\n",
    "LEARNING_RATE = 0.001\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Loads data from the JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        # Adjust based on the actual structure\n",
    "        if \"sentences\" in data and \"sentence\" in data[\"sentences\"]:\n",
    "             return data[\"sentences\"][\"sentence\"]\n",
    "        elif isinstance(data, list): # Handle if data is directly a list of sentences\n",
    "             return data\n",
    "        else:\n",
    "             raise ValueError(\"Cannot find sentence list in the JSON structure\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return []\n",
    "\n",
    "def word_tokenize(sentence):\n",
    "  \"\"\"Tokenizes a Vietnamese sentence.\"\"\"\n",
    "  return underthesea.word_tokenize(sentence)\n",
    "\n",
    "def build_vocab_and_tags(data):\n",
    "    \"\"\"Builds word vocabulary and unified tag vocabulary.\"\"\"\n",
    "    words = Counter()\n",
    "    tags = Counter()\n",
    "    tags['O'] = 1 # Ensure 'O' tag exists\n",
    "\n",
    "    print(\"Building vocabulary and tag set...\")\n",
    "    for sentence_data in data:\n",
    "        if 'text' not in sentence_data or 'aspects' not in sentence_data: continue\n",
    "        text = sentence_data['text']\n",
    "        aspects = sentence_data['aspects']\n",
    "        tokens = word_tokenize(text)\n",
    "        words.update(tokens)\n",
    "\n",
    "        # Generate unified tags temporarily just to count them\n",
    "        token_spans = []\n",
    "        current_pos = 0\n",
    "        for token in tokens:\n",
    "            start = current_pos\n",
    "            end = start + len(token)\n",
    "            token_spans.append((start, end))\n",
    "            current_pos = end + 1\n",
    "\n",
    "        temp_tags = ['O'] * len(tokens)\n",
    "        for aspect in aspects:\n",
    "            try:\n",
    "                aspect_start = int(aspect['from'])\n",
    "                aspect_end = int(aspect['to'])\n",
    "                category = aspect['category'].strip().replace(\" \", \"_\")\n",
    "                polarity = aspect['polarity'].strip()\n",
    "                if not category or not polarity or polarity not in ['positive', 'negative', 'neutral']: continue\n",
    "\n",
    "                b_tag = f\"B-{category}-{polarity}\"\n",
    "                i_tag = f\"I-{category}-{polarity}\"\n",
    "                first_token_in_span = True\n",
    "\n",
    "                for i, (tok_start, tok_end) in enumerate(token_spans):\n",
    "                    token_overlaps = (tok_start >= aspect_start and tok_start < aspect_end) or \\\n",
    "                                     (aspect_start >= tok_start and aspect_start < tok_end)\n",
    "                    if token_overlaps:\n",
    "                        if temp_tags[i] == 'O': # Avoid overwriting (should not happen with no_overlap file)\n",
    "                            if first_token_in_span:\n",
    "                                temp_tags[i] = b_tag\n",
    "                                first_token_in_span = False\n",
    "                            else:\n",
    "                                temp_tags[i] = i_tag\n",
    "            except (ValueError, KeyError):\n",
    "                continue # Skip malformed aspects\n",
    "\n",
    "        tags.update(temp_tags)\n",
    "\n",
    "    # Create word to index mapping\n",
    "    word2idx = {word: i + 2 for i, word in enumerate(words)} # Start from 2\n",
    "    word2idx[PAD_TOKEN] = 0\n",
    "    word2idx[UNK_TOKEN] = 1\n",
    "    idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "    # Create tag to index mapping\n",
    "    tag2idx = {tag: i for i, tag in enumerate(sorted(tags.keys()))}\n",
    "    idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
    "\n",
    "    print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "    print(f\"Tag set size: {len(tag2idx)}\")\n",
    "    # print(\"Tag map:\", tag2idx) # Can be very large, print if needed\n",
    "\n",
    "    return word2idx, idx2word, tag2idx, idx2tag\n",
    "\n",
    "def preprocess_data(data, word2idx, tag2idx, max_len):\n",
    "    \"\"\"Converts sentences and tags to padded sequences of indices.\"\"\"\n",
    "    processed_sentences = []\n",
    "    processed_tags = []\n",
    "\n",
    "    pad_word_idx = word2idx[PAD_TOKEN]\n",
    "    unk_word_idx = word2idx[UNK_TOKEN]\n",
    "    # Use -100 for padding tags, CrossEntropyLoss ignores this index by default\n",
    "    pad_tag_idx = -100\n",
    "\n",
    "    print(\"Preprocessing data into sequences...\")\n",
    "    for sentence_data in data:\n",
    "        if 'text' not in sentence_data or 'aspects' not in sentence_data: continue\n",
    "        text = sentence_data['text']\n",
    "        aspects = sentence_data['aspects']\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Generate unified BIO tags\n",
    "        token_spans = []\n",
    "        current_pos = 0\n",
    "        for token in tokens:\n",
    "            start = current_pos\n",
    "            end = start + len(token)\n",
    "            token_spans.append((start, end))\n",
    "            current_pos = end + 1\n",
    "\n",
    "        bio_tags = ['O'] * len(tokens)\n",
    "        for aspect in aspects:\n",
    "            try:\n",
    "                aspect_start = int(aspect['from'])\n",
    "                aspect_end = int(aspect['to'])\n",
    "                category = aspect['category'].strip().replace(\" \", \"_\")\n",
    "                polarity = aspect['polarity'].strip()\n",
    "                if not category or not polarity or polarity not in ['positive', 'negative', 'neutral']: continue\n",
    "\n",
    "                b_tag = f\"B-{category}-{polarity}\"\n",
    "                i_tag = f\"I-{category}-{polarity}\"\n",
    "                first_token_in_span = True\n",
    "\n",
    "                for i, (tok_start, tok_end) in enumerate(token_spans):\n",
    "                    token_overlaps = (tok_start >= aspect_start and tok_start < aspect_end) or \\\n",
    "                                     (aspect_start >= tok_start and aspect_start < tok_end)\n",
    "                    if token_overlaps:\n",
    "                        if bio_tags[i] == 'O': # Check just in case\n",
    "                            if first_token_in_span:\n",
    "                                bio_tags[i] = b_tag\n",
    "                                first_token_in_span = False\n",
    "                            else:\n",
    "                                bio_tags[i] = i_tag\n",
    "            except (ValueError, KeyError):\n",
    "                continue # Skip malformed aspects\n",
    "\n",
    "        # Convert tokens and tags to indices\n",
    "        sentence_indices = [word2idx.get(token, unk_word_idx) for token in tokens]\n",
    "        tag_indices = [tag2idx.get(tag, tag2idx['O']) for tag in bio_tags] # Default to 'O' if tag not found\n",
    "\n",
    "        # Pad sequences\n",
    "        seq_len = len(sentence_indices)\n",
    "        if seq_len < max_len:\n",
    "            sentence_indices.extend([pad_word_idx] * (max_len - seq_len))\n",
    "            tag_indices.extend([pad_tag_idx] * (max_len - seq_len))\n",
    "        elif seq_len > max_len:\n",
    "            sentence_indices = sentence_indices[:max_len]\n",
    "            tag_indices = tag_indices[:max_len]\n",
    "\n",
    "        processed_sentences.append(sentence_indices)\n",
    "        processed_tags.append(tag_indices)\n",
    "\n",
    "    return torch.tensor(processed_sentences, dtype=torch.long), torch.tensor(processed_tags, dtype=torch.long)\n",
    "\n",
    "\n",
    "# --- 2. Define CNN-LSTM Model ---\n",
    "\n",
    "class CNN_LSTM_Tagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, lstm_hidden_dim,\n",
    "                 cnn_filters, cnn_kernel_sizes, dropout_rate, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.tagset_size = tagset_size\n",
    "\n",
    "        # Embedding Layer (consider loading pre-trained embeddings here)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # CNN Layers\n",
    "        # Input to Conv1d: (batch_size, embedding_dim, sequence_length)\n",
    "        # Output from Conv1d: (batch_size, num_filters, output_length)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=cnn_filters,\n",
    "                      kernel_size=ks)\n",
    "            for ks in cnn_kernel_sizes\n",
    "        ])\n",
    "        cnn_output_dim = cnn_filters * len(cnn_kernel_sizes)\n",
    "\n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # LSTM Layer\n",
    "        # Input to LSTM: (batch_size, sequence_length, cnn_output_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, # <--- Sửa thành embedding_dim (100)\n",
    "                            lstm_hidden_dim,\n",
    "                            num_layers=1, # Can increase layers\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True) # Input/output tensors have batch dim first\n",
    "\n",
    "        # Final Linear Layer (maps LSTM output to tag space)\n",
    "        # Input: (batch_size, seq_len, lstm_hidden_dim * 2) -> Output: (batch_size, seq_len, tagset_size)\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim * 2, tagset_size) # *2 for bidirectional\n",
    "\n",
    "    def forward(self, sentence_indices):\n",
    "        # sentence_indices: (batch_size, seq_len)\n",
    "\n",
    "        # 1. Embedding\n",
    "        embedded = self.dropout(self.embedding(sentence_indices))\n",
    "        # embedded: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # 2. CNN\n",
    "        # Permute for Conv1d: (batch_size, embedding_dim, seq_len)\n",
    "        embedded_permuted = embedded.permute(0, 2, 1)\n",
    "\n",
    "        # Apply convolutions and activation (ReLU)\n",
    "        conved = [torch.relu(conv(embedded_permuted)) for conv in self.convs]\n",
    "        # conved[i]: (batch_size, num_filters, seq_len - kernel_size + 1)\n",
    "\n",
    "        # Max-over-time pooling for each convolution output (or adjust padding in conv)\n",
    "        # To keep sequence length for LSTM, we need padding in Conv1d or careful handling.\n",
    "        # Alternative: Apply LSTM *before* CNN, or directly on embeddings if CNN complexity is too high for now.\n",
    "\n",
    "        # --- Let's simplify: Apply LSTM directly on embeddings for this example ---\n",
    "        # This is a common BiLSTM approach without the CNN complexity.\n",
    "        # If you want CNN, you need to handle the sequence length changes or use padding='same' in Conv1d.\n",
    "\n",
    "        # embedded: (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embedded) # Pass embedded directly\n",
    "        # lstm_out: (batch_size, seq_len, lstm_hidden_dim * 2)\n",
    "\n",
    "        # Apply dropout to LSTM output\n",
    "        lstm_out_dropout = self.dropout(lstm_out)\n",
    "\n",
    "        # 3. Linear Layer\n",
    "        tag_space = self.hidden2tag(lstm_out_dropout)\n",
    "        # tag_space: (batch_size, seq_len, tagset_size)\n",
    "\n",
    "        # We expect CrossEntropyLoss which applies LogSoftmax internally\n",
    "        return tag_space # Return logits\n",
    "\n",
    "# --- Helper Functions for Training/Evaluation ---\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, clip=1.0):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Move batch to device\n",
    "        sentence_in = batch[0].to(device)\n",
    "        targets = batch[1].to(device) # (batch_size, seq_len)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass -> get logits (batch_size, seq_len, tagset_size)\n",
    "        predictions = model(sentence_in)\n",
    "\n",
    "        # Reshape for CrossEntropyLoss: (batch_size * seq_len, tagset_size)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        # Reshape targets: (batch_size * seq_len)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        # Calculate loss (ignore padding tokens automatically via ignore_index in criterion)\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, idx2tag, pad_tag_idx):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            sentence_in = batch[0].to(device)\n",
    "            targets = batch[1].to(device) # (batch_size, seq_len)\n",
    "\n",
    "            predictions = model(sentence_in) # (batch_size, seq_len, tagset_size)\n",
    "\n",
    "            # Calculate loss (on non-padded tokens)\n",
    "            loss = criterion(predictions.view(-1, predictions.shape[-1]), targets.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Get predicted tags (argmax) -> (batch_size, seq_len)\n",
    "            predicted_indices = predictions.argmax(dim=-1)\n",
    "\n",
    "            # Convert indices to tags for seqeval, ignoring padding\n",
    "            for i in range(targets.shape[0]): # Iterate through batch items\n",
    "                true_seq = []\n",
    "                pred_seq = []\n",
    "                for j in range(targets.shape[1]): # Iterate through sequence\n",
    "                    if targets[i, j].item() != pad_tag_idx: # Check if not padding\n",
    "                        true_seq.append(idx2tag[targets[i, j].item()])\n",
    "                        pred_seq.append(idx2tag[predicted_indices[i, j].item()])\n",
    "                if true_seq: # Only add if sequence is not fully padded\n",
    "                    all_trues.append(true_seq)\n",
    "                    all_preds.append(pred_seq)\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    return avg_loss, all_trues, all_preds\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1. Load and process data\n",
    "    raw_data = load_data(DATA_FILE)\n",
    "    if not raw_data: exit()\n",
    "\n",
    "    word2idx, idx2word, tag2idx, idx2tag = build_vocab_and_tags(raw_data)\n",
    "    sentences_idx, tags_idx = preprocess_data(raw_data, word2idx, tag2idx, MAX_LEN)\n",
    "\n",
    "    # 2. Create datasets and dataloaders\n",
    "    dataset = TensorDataset(sentences_idx, tags_idx)\n",
    "\n",
    "    # Split data (using indices)\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        list(range(len(dataset))), test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        train_indices, test_size=0.15, random_state=42 # ~17% validation\n",
    "    )\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(f\"\\nDataLoaders created:\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "    # 3. Initialize model, optimizer, loss\n",
    "    VOCAB_SIZE = len(word2idx)\n",
    "    TAGSET_SIZE = len(tag2idx)\n",
    "    PAD_IDX = word2idx[PAD_TOKEN]\n",
    "    PAD_TAG_IDX = -100 # Important for CrossEntropyLoss ignore_index\n",
    "\n",
    "    # Instantiate the BiLSTM model (removed CNN for simplicity here, rename if needed)\n",
    "    # If you keep CNN, ensure dimensions match or adjust padding in Conv1d\n",
    "    model = CNN_LSTM_Tagger(\n",
    "        VOCAB_SIZE, TAGSET_SIZE, EMBEDDING_DIM, LSTM_HIDDEN_DIM,\n",
    "        CNN_FILTERS, CNN_KERNEL_SIZES, DROPOUT_RATE, PAD_IDX\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TAG_IDX)\n",
    "\n",
    "    # 4. Training Loop\n",
    "    best_val_loss = float('inf')\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_trues, val_preds = evaluate(model, val_loader, criterion, device, idx2tag, PAD_TAG_IDX)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        print(f'\\t Val. Loss: {val_loss:.3f}')\n",
    "\n",
    "        # Save best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'cnn-lstm-aspect-sentiment-best.pt')\n",
    "            print(\"\\t -> Saved best model\")\n",
    "\n",
    "        # Print seqeval report for validation set (optional, can be slow)\n",
    "        try:\n",
    "            # Get labels for report, excluding 'O' maybe? Or include all? Let's include all for now.\n",
    "            all_tags_sorted = sorted(tag2idx.keys(), key=lambda name: (name[1:], name[0]))\n",
    "            val_report = classification_report(val_trues, val_preds, output_dict=False, zero_division=0) # , labels=all_tags_sorted) # Adding labels can make it verbose\n",
    "            print(\"\\nValidation Seqeval Report (sample):\")\n",
    "            # Print only micro avg F1 for brevity during training\n",
    "            f1_micro = classification_report(val_trues, val_preds, output_dict=True, zero_division=0).get('micro avg', {}).get('f1-score', 0)\n",
    "            print(f\"\\t Micro Avg F1: {f1_micro:.3f}\")\n",
    "            # Uncomment below for full report\n",
    "            # print(val_report)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate validation seqeval report: {e}\")\n",
    "\n",
    "    print(\"--- Training Finished ---\")\n",
    "\n",
    "    # 5. Evaluate on Test Set\n",
    "    print(\"\\n--- Evaluating on Test Set ---\")\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('cnn-lstm-aspect-sentiment-best.pt'))\n",
    "    test_loss, test_trues, test_preds = evaluate(model, test_loader, criterion, device, idx2tag, PAD_TAG_IDX)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.3f}')\n",
    "\n",
    "    try:\n",
    "        print(\"\\nTest Set Seqeval Classification Report:\")\n",
    "        test_report = classification_report(test_trues, test_preds, output_dict=False, zero_division=0)\n",
    "        print(test_report)\n",
    "    except Exception as e:\n",
    "         print(f\"Could not generate test seqeval report: {e}\")\n",
    "\n",
    "\n",
    "    # --- 6. Prediction Example ---\n",
    "    def predict_sentence(sentence, model, word2idx, idx2tag, device, max_len):\n",
    "        model.eval()\n",
    "        tokens = word_tokenize(sentence)\n",
    "        if not tokens: return [], []\n",
    "\n",
    "        # Convert to indices, pad\n",
    "        unk_idx = word2idx[UNK_TOKEN]\n",
    "        pad_idx = word2idx[PAD_TOKEN]\n",
    "        indices = [word2idx.get(t, unk_idx) for t in tokens]\n",
    "        orig_len = len(indices)\n",
    "\n",
    "        if len(indices) < max_len:\n",
    "            indices.extend([pad_idx] * (max_len - len(indices)))\n",
    "        elif len(indices) > max_len:\n",
    "            indices = indices[:max_len]\n",
    "            orig_len = max_len # Adjust original length if truncated\n",
    "\n",
    "        # Convert to tensor and add batch dimension\n",
    "        sentence_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(sentence_tensor) # (1, seq_len, tagset_size)\n",
    "\n",
    "        predicted_indices = predictions.argmax(dim=-1)[0].cpu().numpy() # (seq_len)\n",
    "\n",
    "        # Convert indices back to tags, up to original length\n",
    "        predicted_tags = [idx2tag[idx] for idx in predicted_indices[:orig_len]]\n",
    "\n",
    "        return tokens[:orig_len], predicted_tags # Return original tokens and predicted tags\n",
    "\n",
    "    print(\"\\n--- Prediction Example ---\")\n",
    "    test_sentence = \"giáo viên nhiệt tình nhưng cơ sở vật chất cần cải thiện .\"\n",
    "    pred_tokens, pred_tags = predict_sentence(test_sentence, model, word2idx, idx2tag, device, MAX_LEN)\n",
    "    print(f\"Sentence: {test_sentence}\")\n",
    "    print(f\"Tokens: {pred_tokens}\")\n",
    "    print(f\"Predicted Tags: {pred_tags}\")\n",
    "\n",
    "    # You can add the tag grouping logic here if needed (similar to CRF example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
