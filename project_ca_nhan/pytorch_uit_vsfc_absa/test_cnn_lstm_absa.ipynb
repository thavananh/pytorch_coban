{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b60d777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "File loaded successfully.\n",
      "Original Columns: Index(['id', 'Review', 'Sentence Component', 'aspect_text', 'aspect',\n",
      "       'sentiment_text', 'sentiment'],\n",
      "      dtype='object')\n",
      "Data shape after selecting columns and dropping NaN: (7778, 3)\n",
      "Data shape after removing empty/whitespace reviews: (7778, 3)\n",
      "Actual vocabulary size: 2797\n",
      "Sentiment Labels: {'Negative': 0, 'Positive': 1, 'Neutral': 2}\n",
      "Number of sentiment classes: 3\n",
      "Aspect Labels: {'Teaching quality': 0, 'Workload': 1, 'General review': 2, 'Course information': 3, 'Support from lecturers': 4, 'Test and evaluation': 5, 'Learning environment': 6, 'Organization and management': 7}\n",
      "Number of aspect classes: 8\n",
      "Train size: 6222, Validation size: 1556\n",
      "CNN_LSTM_MTL(\n",
      "  (embedding): Embedding(2797, 128, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(128, 64, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (2): Conv1d(128, 64, kernel_size=(4,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
      "  (fc_sentiment): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (fc_aspect): Linear(in_features=256, out_features=8, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "The model has 633,419 trainable parameters\n",
      "\n",
      "Starting Training...\n",
      "Epoch: 01 | Time: 0m 12s\n",
      "\tTrain Loss: 2.419 | Train Acc Sent: 61.94% | Train Acc Aspect: 40.06%\n",
      "\t Val. Loss: 2.219 |  Val. Acc Sent: 70.38% |  Val. Acc Aspect: 38.30%\n",
      "Epoch: 02 | Time: 0m 16s\n",
      "\tTrain Loss: 2.098 | Train Acc Sent: 72.93% | Train Acc Aspect: 44.07%\n",
      "\t Val. Loss: 2.037 |  Val. Acc Sent: 80.28% |  Val. Acc Aspect: 45.05%\n",
      "Epoch: 03 | Time: 0m 13s\n",
      "\tTrain Loss: 1.946 | Train Acc Sent: 77.39% | Train Acc Aspect: 48.03%\n",
      "\t Val. Loss: 1.899 |  Val. Acc Sent: 80.84% |  Val. Acc Aspect: 47.29%\n",
      "Epoch: 04 | Time: 0m 16s\n",
      "\tTrain Loss: 1.846 | Train Acc Sent: 79.92% | Train Acc Aspect: 51.00%\n",
      "\t Val. Loss: 1.820 |  Val. Acc Sent: 83.09% |  Val. Acc Aspect: 48.46%\n",
      "Epoch: 05 | Time: 0m 13s\n",
      "\tTrain Loss: 1.766 | Train Acc Sent: 81.30% | Train Acc Aspect: 53.67%\n",
      "\t Val. Loss: 1.747 |  Val. Acc Sent: 83.71% |  Val. Acc Aspect: 54.12%\n",
      "Epoch: 06 | Time: 0m 16s\n",
      "\tTrain Loss: 1.679 | Train Acc Sent: 82.14% | Train Acc Aspect: 56.72%\n",
      "\t Val. Loss: 1.693 |  Val. Acc Sent: 84.65% |  Val. Acc Aspect: 56.01%\n",
      "Epoch: 07 | Time: 0m 14s\n",
      "\tTrain Loss: 1.622 | Train Acc Sent: 82.64% | Train Acc Aspect: 56.99%\n",
      "\t Val. Loss: 1.632 |  Val. Acc Sent: 85.34% |  Val. Acc Aspect: 56.78%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 343\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m    341\u001b[39m     start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     train_loss, train_acc_sent, train_acc_aspect = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_sentiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_aspect\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m     valid_loss, valid_acc_sent, valid_acc_aspect = evaluate_epoch(\n\u001b[32m    347\u001b[39m         model, val_loader, criterion_sentiment, criterion_aspect\n\u001b[32m    348\u001b[39m     )\n\u001b[32m    350\u001b[39m     end_time = time.time()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 295\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, iterator, optimizer, criterion_sentiment, criterion_aspect)\u001b[39m\n\u001b[32m    292\u001b[39m acc_aspect = calculate_accuracy(predictions_aspect, aspect_labels)\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Backpropagate the combined loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m optimizer.step()\n\u001b[32m    298\u001b[39m epoch_loss += total_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/Ai_ENV/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/Ai_ENV/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/Ai_ENV/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from underthesea import word_tokenize\n",
    "import time # Added for timing epochs\n",
    "\n",
    "# --- 1. Setup & Configuration ---\n",
    "FILE_PATH = 'combined_cleaned_file.csv'\n",
    "TEXT_COLUMN = 'Review'\n",
    "SENTIMENT_COLUMN = 'sentiment'\n",
    "ASPECT_COLUMN = 'aspect' # New column for aspect\n",
    "\n",
    "# Hyperparameters (mostly same, adjusted VOCAB_SIZE slightly based on previous run)\n",
    "VOCAB_SIZE = 3000 # Adjusted based on previous output (2797) - can increase if needed\n",
    "MAX_LEN = 100\n",
    "EMBEDDING_DIM = 128\n",
    "NUM_FILTERS = 64\n",
    "FILTER_SIZES = [2, 3, 4]\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "EPOCHS = 10 # Can adjust as needed\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Keep CPU for now, or switch back to CUDA if CPU run was successful\n",
    "DEVICE = torch.device('cpu') # Forcing CPU as requested before\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 2. Data Loading and Initial Cleaning ---\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "    print(\"File loaded successfully.\")\n",
    "    print(\"Original Columns:\", df.columns)\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [TEXT_COLUMN, SENTIMENT_COLUMN, ASPECT_COLUMN]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Required columns {missing} not found in the file.\")\n",
    "\n",
    "    # Drop rows with NaN in essential columns AFTER checking they exist\n",
    "    df = df.dropna(subset=required_cols)\n",
    "    df = df[required_cols].copy() # Keep only necessary columns\n",
    "    print(f\"Data shape after selecting columns and dropping NaN: {df.shape}\")\n",
    "\n",
    "    # (Optional but recommended) Filter empty/whitespace reviews\n",
    "    df = df[df[TEXT_COLUMN].str.strip().astype(bool)]\n",
    "    print(f\"Data shape after removing empty/whitespace reviews: {df.shape}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {FILE_PATH}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Text Preprocessing and Vocabulary Building ---\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    # Add any other cleaning steps if needed (e.g., removing punctuation, numbers)\n",
    "    # text = re.sub(r'[^\\w\\s]', '', text) # Example: remove punctuation\n",
    "    return text\n",
    "\n",
    "# Tách từ tiếng Việt\n",
    "def tokenize_vi(text):\n",
    "    try:\n",
    "        # Ensure text is a string\n",
    "        return word_tokenize(str(text), format=\"text\").split()\n",
    "    except Exception as e:\n",
    "        # Handle potential errors during tokenization (e.g., empty strings after cleaning)\n",
    "        # print(f\"Warning: Tokenization failed for text: '{text}'. Error: {e}\") # Optional warning\n",
    "        return []\n",
    "\n",
    "df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(preprocess_text)\n",
    "df['tokens'] = df[TEXT_COLUMN].apply(tokenize_vi)\n",
    "\n",
    "# Build Vocabulary (based only on training data after split is better, but simpler for now)\n",
    "word_counts = Counter(word for tokens in df['tokens'] for word in tokens)\n",
    "vocab = [word for word, count in word_counts.most_common(VOCAB_SIZE - 2)] # Reserve 0 for PAD, 1 for UNK\n",
    "word_to_ix = {word: i+2 for i, word in enumerate(vocab)}\n",
    "word_to_ix['<PAD>'] = 0\n",
    "word_to_ix['<UNK>'] = 1\n",
    "actual_vocab_size = len(word_to_ix)\n",
    "print(f\"Actual vocabulary size: {actual_vocab_size}\")\n",
    "\n",
    "# --- 4. Data Encoding (Tokens to Indices, Labels to Indices) ---\n",
    "def tokens_to_indices(tokens, word_to_ix, max_len):\n",
    "    indices = [word_to_ix.get(token, word_to_ix['<UNK>']) for token in tokens]\n",
    "    indices = indices[:max_len] # Truncate\n",
    "    padded_indices = indices + [word_to_ix['<PAD>']] * (max_len - len(indices)) # Pad\n",
    "    return padded_indices\n",
    "\n",
    "df['indices'] = df['tokens'].apply(lambda x: tokens_to_indices(x, word_to_ix, MAX_LEN))\n",
    "\n",
    "# Encode Sentiment Labels\n",
    "unique_sentiments = df[SENTIMENT_COLUMN].unique()\n",
    "sentiment_to_ix = {label: i for i, label in enumerate(unique_sentiments)}\n",
    "ix_to_sentiment = {i: label for label, i in sentiment_to_ix.items()}\n",
    "num_sentiment_classes = len(unique_sentiments)\n",
    "print(f\"Sentiment Labels: {sentiment_to_ix}\")\n",
    "print(f\"Number of sentiment classes: {num_sentiment_classes}\")\n",
    "df['sentiment_encoded'] = df[SENTIMENT_COLUMN].map(sentiment_to_ix)\n",
    "\n",
    "# Encode Aspect Labels\n",
    "unique_aspects = df[ASPECT_COLUMN].unique()\n",
    "aspect_to_ix = {label: i for i, label in enumerate(unique_aspects)}\n",
    "ix_to_aspect = {i: label for label, i in aspect_to_ix.items()}\n",
    "num_aspect_classes = len(unique_aspects)\n",
    "print(f\"Aspect Labels: {aspect_to_ix}\")\n",
    "print(f\"Number of aspect classes: {num_aspect_classes}\")\n",
    "df['aspect_encoded'] = df[ASPECT_COLUMN].map(aspect_to_ix)\n",
    "\n",
    "\n",
    "# --- 5. Data Splitting and DataLoader Creation ---\n",
    "X = list(df['indices'])\n",
    "y_sentiment = list(df['sentiment_encoded'])\n",
    "y_aspect = list(df['aspect_encoded'])\n",
    "\n",
    "# Split data - stratify by sentiment (or aspect, or combination if more complex needed)\n",
    "X_train, X_val, y_sentiment_train, y_sentiment_val, y_aspect_train, y_aspect_val = train_test_split(\n",
    "    X, y_sentiment, y_aspect, test_size=0.2, random_state=42, stratify=y_sentiment # Stratify by sentiment\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}\")\n",
    "\n",
    "# Create Dataset\n",
    "class AspectSentimentDataset(Dataset):\n",
    "    def __init__(self, indices, sentiment_labels, aspect_labels):\n",
    "        self.indices = indices\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.aspect_labels = aspect_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.indices[idx], dtype=torch.long),\n",
    "            torch.tensor(self.sentiment_labels[idx], dtype=torch.long),\n",
    "            torch.tensor(self.aspect_labels[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "train_dataset = AspectSentimentDataset(X_train, y_sentiment_train, y_aspect_train)\n",
    "val_dataset = AspectSentimentDataset(X_val, y_sentiment_val, y_aspect_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- 6. Model Definition (CNN_LSTM for Multi-Task Learning) ---\n",
    "class CNN_LSTM_MTL(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes,\n",
    "                 lstm_hidden_dim, num_sentiment_classes, num_aspect_classes, # Two output dims\n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=fs,\n",
    "                      padding=(fs - 1) // 2) # Maintain length approx.\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        lstm_input_size = num_filters # Input to LSTM is output of one Conv layer\n",
    "        self.lstm = nn.LSTM(lstm_input_size,\n",
    "                            lstm_hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Calculate input dimension for the fully connected layers\n",
    "        fc_input_dim = lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim\n",
    "\n",
    "        # Output layer for Sentiment Prediction\n",
    "        self.fc_sentiment = nn.Linear(fc_input_dim, num_sentiment_classes)\n",
    "\n",
    "        # Output layer for Aspect Prediction\n",
    "        self.fc_aspect = nn.Linear(fc_input_dim, num_aspect_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded = [batch size, seq len, emb dim]\n",
    "\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        # embedded = [batch size, emb dim, seq len]\n",
    "\n",
    "        # Apply first convolution layer (as an example, could also combine features)\n",
    "        conv_out = self.convs[0](embedded)\n",
    "        # conv_out = [batch size, num filters, seq len]\n",
    "        conved = torch.relu(conv_out)\n",
    "\n",
    "        # Prepare for LSTM\n",
    "        conved = conved.permute(0, 2, 1)\n",
    "        # conved = [batch size, seq len, num filters]\n",
    "\n",
    "        # LSTM layer\n",
    "        lstm_output, (hidden, cell) = self.lstm(conved)\n",
    "        # lstm_output = [batch size, seq len, hidden dim * num directions]\n",
    "        # hidden = [num layers * num directions, batch size, hidden dim]\n",
    "\n",
    "        # Get final hidden state (concat directions if bidirectional)\n",
    "        if self.lstm.bidirectional:\n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=-1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        # hidden = [batch size, hidden dim * num directions]\n",
    "\n",
    "        # --- Multi-Task Outputs ---\n",
    "        output_sentiment = self.fc_sentiment(hidden) # Output for sentiment\n",
    "        output_aspect = self.fc_aspect(hidden)       # Output for aspect\n",
    "        # output_sentiment = [batch size, num_sentiment_classes]\n",
    "        # output_aspect = [batch size, num_aspect_classes]\n",
    "\n",
    "        return output_sentiment, output_aspect # Return both outputs\n",
    "\n",
    "# --- 7. Initialization ---\n",
    "PAD_IDX = word_to_ix['<PAD>']\n",
    "model = CNN_LSTM_MTL(\n",
    "    vocab_size=actual_vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
    "    num_sentiment_classes=num_sentiment_classes, # Pass sentiment classes\n",
    "    num_aspect_classes=num_aspect_classes,       # Pass aspect classes\n",
    "    n_layers=N_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# --- 8. Training Setup ---\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# Use separate criteria if needed, but CrossEntropy works for both classification tasks\n",
    "criterion_sentiment = nn.CrossEntropyLoss().to(DEVICE)\n",
    "criterion_aspect = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "def calculate_accuracy(preds, y):\n",
    "    \"\"\"Calculates accuracy for a single task\"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "# --- 9. Training and Evaluation Loops (Modified for MTL) ---\n",
    "def train_epoch(model, iterator, optimizer, criterion_sentiment, criterion_aspect):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc_sentiment = 0\n",
    "    epoch_acc_aspect = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        text, sentiment_labels, aspect_labels = batch # Unpack all three\n",
    "        text = text.to(DEVICE)\n",
    "        sentiment_labels = sentiment_labels.to(DEVICE)\n",
    "        aspect_labels = aspect_labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass - returns two outputs\n",
    "        predictions_sentiment, predictions_aspect = model(text)\n",
    "\n",
    "        # Calculate losses for each task\n",
    "        loss_sentiment = criterion_sentiment(predictions_sentiment, sentiment_labels)\n",
    "        loss_aspect = criterion_aspect(predictions_aspect, aspect_labels)\n",
    "\n",
    "        # Combine losses (simple sum, can use weighting alpha*loss_sent + (1-alpha)*loss_aspect if needed)\n",
    "        total_loss = loss_sentiment + loss_aspect\n",
    "\n",
    "        # Calculate accuracies\n",
    "        acc_sentiment = calculate_accuracy(predictions_sentiment, sentiment_labels)\n",
    "        acc_aspect = calculate_accuracy(predictions_aspect, aspect_labels)\n",
    "\n",
    "        # Backpropagate the combined loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_acc_sentiment += acc_sentiment.item()\n",
    "        epoch_acc_aspect += acc_aspect.item()\n",
    "\n",
    "    return (epoch_loss / len(iterator),\n",
    "            epoch_acc_sentiment / len(iterator),\n",
    "            epoch_acc_aspect / len(iterator))\n",
    "\n",
    "def evaluate_epoch(model, iterator, criterion_sentiment, criterion_aspect):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc_sentiment = 0\n",
    "    epoch_acc_aspect = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, sentiment_labels, aspect_labels = batch\n",
    "            text = text.to(DEVICE)\n",
    "            sentiment_labels = sentiment_labels.to(DEVICE)\n",
    "            aspect_labels = aspect_labels.to(DEVICE)\n",
    "\n",
    "            predictions_sentiment, predictions_aspect = model(text)\n",
    "\n",
    "            loss_sentiment = criterion_sentiment(predictions_sentiment, sentiment_labels)\n",
    "            loss_aspect = criterion_aspect(predictions_aspect, aspect_labels)\n",
    "            total_loss = loss_sentiment + loss_aspect\n",
    "\n",
    "            acc_sentiment = calculate_accuracy(predictions_sentiment, sentiment_labels)\n",
    "            acc_aspect = calculate_accuracy(predictions_aspect, aspect_labels)\n",
    "\n",
    "            epoch_loss += total_loss.item()\n",
    "            epoch_acc_sentiment += acc_sentiment.item()\n",
    "            epoch_acc_aspect += acc_aspect.item()\n",
    "\n",
    "    return (epoch_loss / len(iterator),\n",
    "            epoch_acc_sentiment / len(iterator),\n",
    "            epoch_acc_aspect / len(iterator))\n",
    "\n",
    "# --- 10. Main Training Loop ---\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc_sent, train_acc_aspect = train_epoch(\n",
    "        model, train_loader, optimizer, criterion_sentiment, criterion_aspect\n",
    "    )\n",
    "    valid_loss, valid_acc_sent, valid_acc_aspect = evaluate_epoch(\n",
    "        model, val_loader, criterion_sentiment, criterion_aspect\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    # Save the best model based on combined validation loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'cnn_lstm_mtl_model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc Sent: {train_acc_sent*100:.2f}% | Train Acc Aspect: {train_acc_aspect*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc Sent: {valid_acc_sent*100:.2f}% |  Val. Acc Aspect: {valid_acc_aspect*100:.2f}%')\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 11. Evaluation and Prediction (Modified for MTL) ---\n",
    "# Load the best model\n",
    "try:\n",
    "    model.load_state_dict(torch.load('cnn_lstm_mtl_model.pt', map_location=DEVICE)) # Ensure map_location if loading between CPU/GPU\n",
    "    print(\"\\nLoaded best model weights.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nWarning: Model file 'cnn_lstm_mtl_model.pt' not found. Using model with weights from last epoch.\")\n",
    "except Exception as e:\n",
    "     print(f\"\\nWarning: Could not load model weights. Error: {e}. Using model with weights from last epoch.\")\n",
    "\n",
    "\n",
    "# Final evaluation\n",
    "final_val_loss, final_val_acc_sent, final_val_acc_aspect = evaluate_epoch(\n",
    "    model, val_loader, criterion_sentiment, criterion_aspect\n",
    ")\n",
    "print(f'\\nFinal Validation Loss: {final_val_loss:.3f}')\n",
    "print(f'Final Validation Sentiment Accuracy: {final_val_acc_sent*100:.2f}%')\n",
    "print(f'Final Validation Aspect Accuracy: {final_val_acc_aspect*100:.2f}%')\n",
    "\n",
    "# Prediction function for both tasks\n",
    "def predict_aspect_sentiment(sentence, model, word_to_ix, max_len, device, ix_to_sentiment, ix_to_aspect):\n",
    "    model.eval()\n",
    "    processed_text = preprocess_text(sentence)\n",
    "    tokens = tokenize_vi(processed_text)\n",
    "    indexed = tokens_to_indices(tokens, word_to_ix, max_len)\n",
    "    tensor = torch.LongTensor(indexed).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction_sentiment, prediction_aspect = model(tensor) # Get both predictions\n",
    "\n",
    "    probs_sentiment = torch.softmax(prediction_sentiment, dim=1)\n",
    "    probs_aspect = torch.softmax(prediction_aspect, dim=1)\n",
    "\n",
    "    # Sentiment Prediction\n",
    "    predicted_class_sentiment = prediction_sentiment.argmax(dim=1).item()\n",
    "    predicted_label_sentiment = ix_to_sentiment[predicted_class_sentiment]\n",
    "    confidence_sentiment = probs_sentiment.max().item()\n",
    "\n",
    "    # Aspect Prediction\n",
    "    predicted_class_aspect = prediction_aspect.argmax(dim=1).item()\n",
    "    predicted_label_aspect = ix_to_aspect[predicted_class_aspect]\n",
    "    confidence_aspect = probs_aspect.max().item()\n",
    "\n",
    "    return predicted_label_sentiment, confidence_sentiment, predicted_label_aspect, confidence_aspect\n",
    "\n",
    "# Example Predictions\n",
    "print(\"\\n--- Example Predictions ---\")\n",
    "test_sentences = [\n",
    "    \"Giảng viên dạy rất hay và nhiệt tình.\",\n",
    "    \"Nội dung môn học quá cũ, không cập nhật.\",\n",
    "    \"Tài liệu học tập đầy đủ.\",\n",
    "    \"Phòng học quá nóng và thiếu ánh sáng.\",\n",
    "    \"Thầy cô hỗ trợ sinh viên rất tốt ngoài giờ học.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    pred_sent, conf_sent, pred_aspect, conf_aspect = predict_aspect_sentiment(\n",
    "        sentence, model, word_to_ix, MAX_LEN, DEVICE, ix_to_sentiment, ix_to_aspect\n",
    "    )\n",
    "    print(f'\\nSentence:  \"{sentence}\"')\n",
    "    print(f'Predicted: Aspect = {pred_aspect} (Conf: {conf_aspect:.4f}), Sentiment = {pred_sent} (Conf: {conf_sent:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a9eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "File loaded successfully.\n",
      "Data shape after loading and initial cleaning: (7778, 3)\n",
      "Actual vocabulary size: 2797\n",
      "Sentiment Labels: {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
      "Aspect Labels: {'Course information': 0, 'General review': 1, 'Learning environment': 2, 'Organization and management': 3, 'Support from lecturers': 4, 'Teaching quality': 5, 'Test and evaluation': 6, 'Workload': 7}\n",
      "Train size: 4977\n",
      "Validation size: 1245\n",
      "Test size: 1556\n",
      "CNN_LSTM_MTL(\n",
      "  (embedding): Embedding(2797, 128, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(128, 64, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (2): Conv1d(128, 64, kernel_size=(4,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
      "  (fc_sentiment): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (fc_aspect): Linear(in_features=256, out_features=8, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "The model has 633,419 trainable parameters\n",
      "\n",
      "Starting Training...\n",
      "Epoch: 01 | Time: 0m 6s\n",
      "\tTrain Loss: 2.477 | Train Acc Sent: 61.64% | Train Acc Aspect: 38.13%\n",
      "\t Val. Loss: 2.179 |  Val. Acc Sent: 72.14% |  Val. Acc Aspect: 40.13%\n",
      "\tValidation loss decreased (2.179). Saving model...\n",
      "Epoch: 02 | Time: 0m 3s\n",
      "\tTrain Loss: 2.133 | Train Acc Sent: 72.88% | Train Acc Aspect: 41.94%\n",
      "\t Val. Loss: 2.043 |  Val. Acc Sent: 76.79% |  Val. Acc Aspect: 46.54%\n",
      "\tValidation loss decreased (2.043). Saving model...\n",
      "Epoch: 03 | Time: 0m 6s\n",
      "\tTrain Loss: 1.999 | Train Acc Sent: 76.69% | Train Acc Aspect: 46.01%\n",
      "\t Val. Loss: 1.932 |  Val. Acc Sent: 79.15% |  Val. Acc Aspect: 48.88%\n",
      "\tValidation loss decreased (1.932). Saving model...\n",
      "Epoch: 04 | Time: 0m 6s\n",
      "\tTrain Loss: 1.901 | Train Acc Sent: 78.91% | Train Acc Aspect: 48.90%\n",
      "\t Val. Loss: 1.848 |  Val. Acc Sent: 80.40% |  Val. Acc Aspect: 51.77%\n",
      "\tValidation loss decreased (1.848). Saving model...\n",
      "Epoch: 05 | Time: 0m 6s\n",
      "\tTrain Loss: 1.803 | Train Acc Sent: 81.36% | Train Acc Aspect: 52.20%\n",
      "\t Val. Loss: 1.776 |  Val. Acc Sent: 81.72% |  Val. Acc Aspect: 54.82%\n",
      "\tValidation loss decreased (1.776). Saving model...\n",
      "Epoch: 06 | Time: 0m 7s\n",
      "\tTrain Loss: 1.724 | Train Acc Sent: 82.11% | Train Acc Aspect: 53.60%\n",
      "\t Val. Loss: 1.720 |  Val. Acc Sent: 81.70% |  Val. Acc Aspect: 56.23%\n",
      "\tValidation loss decreased (1.720). Saving model...\n",
      "Epoch: 07 | Time: 0m 3s\n",
      "\tTrain Loss: 1.656 | Train Acc Sent: 83.01% | Train Acc Aspect: 56.60%\n",
      "\t Val. Loss: 1.666 |  Val. Acc Sent: 83.18% |  Val. Acc Aspect: 56.20%\n",
      "\tValidation loss decreased (1.666). Saving model...\n",
      "Epoch: 08 | Time: 0m 7s\n",
      "\tTrain Loss: 1.596 | Train Acc Sent: 83.74% | Train Acc Aspect: 57.52%\n",
      "\t Val. Loss: 1.658 |  Val. Acc Sent: 83.28% |  Val. Acc Aspect: 57.96%\n",
      "\tValidation loss decreased (1.658). Saving model...\n",
      "Epoch: 09 | Time: 0m 7s\n",
      "\tTrain Loss: 1.560 | Train Acc Sent: 83.66% | Train Acc Aspect: 59.22%\n",
      "\t Val. Loss: 1.621 |  Val. Acc Sent: 82.79% |  Val. Acc Aspect: 57.29%\n",
      "\tValidation loss decreased (1.621). Saving model...\n",
      "Epoch: 10 | Time: 0m 7s\n",
      "\tTrain Loss: 1.504 | Train Acc Sent: 84.58% | Train Acc Aspect: 60.60%\n",
      "\t Val. Loss: 1.594 |  Val. Acc Sent: 82.95% |  Val. Acc Aspect: 58.04%\n",
      "\tValidation loss decreased (1.594). Saving model...\n",
      "Epoch: 11 | Time: 0m 4s\n",
      "\tTrain Loss: 1.461 | Train Acc Sent: 85.25% | Train Acc Aspect: 62.41%\n",
      "\t Val. Loss: 1.573 |  Val. Acc Sent: 83.26% |  Val. Acc Aspect: 59.62%\n",
      "\tValidation loss decreased (1.573). Saving model...\n",
      "Epoch: 12 | Time: 0m 7s\n",
      "\tTrain Loss: 1.433 | Train Acc Sent: 85.78% | Train Acc Aspect: 62.38%\n",
      "\t Val. Loss: 1.557 |  Val. Acc Sent: 83.11% |  Val. Acc Aspect: 59.95%\n",
      "\tValidation loss decreased (1.557). Saving model...\n",
      "Epoch: 13 | Time: 0m 6s\n",
      "\tTrain Loss: 1.402 | Train Acc Sent: 85.50% | Train Acc Aspect: 63.24%\n",
      "\t Val. Loss: 1.567 |  Val. Acc Sent: 82.79% |  Val. Acc Aspect: 60.18%\n",
      "\tValidation loss did not decrease. (1/3)\n",
      "Epoch: 14 | Time: 0m 7s\n",
      "\tTrain Loss: 1.370 | Train Acc Sent: 86.68% | Train Acc Aspect: 63.85%\n",
      "\t Val. Loss: 1.542 |  Val. Acc Sent: 83.73% |  Val. Acc Aspect: 59.91%\n",
      "\tValidation loss decreased (1.542). Saving model...\n",
      "Epoch: 15 | Time: 0m 7s\n",
      "\tTrain Loss: 1.334 | Train Acc Sent: 86.61% | Train Acc Aspect: 65.10%\n",
      "\t Val. Loss: 1.532 |  Val. Acc Sent: 83.81% |  Val. Acc Aspect: 60.09%\n",
      "\tValidation loss decreased (1.532). Saving model...\n",
      "Epoch: 16 | Time: 0m 3s\n",
      "\tTrain Loss: 1.298 | Train Acc Sent: 87.36% | Train Acc Aspect: 65.69%\n",
      "\t Val. Loss: 1.521 |  Val. Acc Sent: 83.50% |  Val. Acc Aspect: 59.70%\n",
      "\tValidation loss decreased (1.521). Saving model...\n",
      "Epoch: 17 | Time: 0m 7s\n",
      "\tTrain Loss: 1.273 | Train Acc Sent: 87.96% | Train Acc Aspect: 66.52%\n",
      "\t Val. Loss: 1.505 |  Val. Acc Sent: 84.29% |  Val. Acc Aspect: 60.43%\n",
      "\tValidation loss decreased (1.505). Saving model...\n",
      "Epoch: 18 | Time: 0m 7s\n",
      "\tTrain Loss: 1.272 | Train Acc Sent: 87.50% | Train Acc Aspect: 67.08%\n",
      "\t Val. Loss: 1.523 |  Val. Acc Sent: 84.43% |  Val. Acc Aspect: 59.85%\n",
      "\tValidation loss did not decrease. (1/3)\n",
      "Epoch: 19 | Time: 0m 7s\n",
      "\tTrain Loss: 1.231 | Train Acc Sent: 87.46% | Train Acc Aspect: 67.06%\n",
      "\t Val. Loss: 1.537 |  Val. Acc Sent: 83.11% |  Val. Acc Aspect: 60.79%\n",
      "\tValidation loss did not decrease. (2/3)\n",
      "Epoch: 20 | Time: 0m 4s\n",
      "\tTrain Loss: 1.220 | Train Acc Sent: 87.86% | Train Acc Aspect: 67.76%\n",
      "\t Val. Loss: 1.510 |  Val. Acc Sent: 84.20% |  Val. Acc Aspect: 59.93%\n",
      "\tValidation loss did not decrease. (3/3)\n",
      "\n",
      "Early stopping triggered after 20 epochs.\n",
      "\n",
      "Loading best model from 'cnn_lstm_mtl_best_model.pt' for test evaluation...\n",
      "\n",
      "--- Test Set Evaluation Results ---\n",
      "Test Loss: 1.461\n",
      "Test Sentiment Accuracy: 84.41%\n",
      "Test Aspect Accuracy: 62.74%\n",
      "\n",
      "--- Sentiment Classification Report (Test Set) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.85      0.81      0.83       502\n",
      "     Neutral       0.60      0.66      0.63       222\n",
      "    Positive       0.91      0.91      0.91       832\n",
      "\n",
      "    accuracy                           0.84      1556\n",
      "   macro avg       0.79      0.79      0.79      1556\n",
      "weighted avg       0.85      0.84      0.84      1556\n",
      "\n",
      "\n",
      "--- Sentiment Confusion Matrix (Test Set) ---\n",
      "Labels: ['Negative', 'Neutral', 'Positive']\n",
      "[[406  58  38]\n",
      " [ 38 147  37]\n",
      " [ 35  39 758]]\n",
      "\n",
      "--- Aspect Classification Report (Test Set) ---\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "         Course information       0.55      0.59      0.57       139\n",
      "             General review       0.54      0.30      0.39       109\n",
      "       Learning environment       0.76      0.76      0.76        89\n",
      "Organization and management       0.50      0.55      0.52        98\n",
      "     Support from lecturers       0.61      0.55      0.58       386\n",
      "           Teaching quality       0.69      0.76      0.72       615\n",
      "        Test and evaluation       0.37      0.26      0.31        50\n",
      "                   Workload       0.51      0.63      0.56        70\n",
      "\n",
      "                   accuracy                           0.63      1556\n",
      "                  macro avg       0.57      0.55      0.55      1556\n",
      "               weighted avg       0.62      0.63      0.62      1556\n",
      "\n",
      "\n",
      "--- Aspect Confusion Matrix (Test Set) ---\n",
      "Labels: ['Course information', 'General review', 'Learning environment', 'Organization and management', 'Support from lecturers', 'Teaching quality', 'Test and evaluation', 'Workload']\n",
      "[[ 82   7   3   3  10  18   4  12]\n",
      " [ 10  33   2   6  18  35   3   2]\n",
      " [  4   4  68   4   2   5   1   1]\n",
      " [  8   3   8  54   5  11   4   5]\n",
      " [  4   6   1  17 211 132   6   9]\n",
      " [ 25   5   3  16  85 468   4   9]\n",
      " [  9   3   4   5   9   3  13   4]\n",
      " [  8   0   1   3   6   8   0  44]]\n",
      "\n",
      "--- Example Predictions (using best model) ---\n",
      "\n",
      "Sentence:  \"Giảng viên dạy rất hay và nhiệt tình.\"\n",
      "Predicted: Aspect = Teaching quality (Conf: 0.7408), Sentiment = Positive (Conf: 0.9997)\n",
      "\n",
      "Sentence:  \"Nội dung môn học quá cũ, không cập nhật.\"\n",
      "Predicted: Aspect = Course information (Conf: 0.8989), Sentiment = Negative (Conf: 0.9680)\n",
      "\n",
      "Sentence:  \"Tài liệu học tập đầy đủ.\"\n",
      "Predicted: Aspect = Course information (Conf: 0.7876), Sentiment = Positive (Conf: 0.8886)\n",
      "\n",
      "Sentence:  \"Phòng học quá nóng và thiếu ánh sáng.\"\n",
      "Predicted: Aspect = Learning environment (Conf: 0.9012), Sentiment = Negative (Conf: 0.9840)\n",
      "\n",
      "Sentence:  \"Thầy cô hỗ trợ sinh viên rất tốt ngoài giờ học.\"\n",
      "Predicted: Aspect = Support from lecturers (Conf: 0.7192), Sentiment = Positive (Conf: 0.9968)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Added imports for evaluation metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from underthesea import word_tokenize\n",
    "import time\n",
    "\n",
    "# --- 1. Setup & Configuration ---\n",
    "FILE_PATH = 'combined_cleaned_file.csv'\n",
    "TEXT_COLUMN = 'Review'\n",
    "SENTIMENT_COLUMN = 'sentiment'\n",
    "ASPECT_COLUMN = 'aspect'\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 3000\n",
    "MAX_LEN = 100\n",
    "EMBEDDING_DIM = 128\n",
    "NUM_FILTERS = 64\n",
    "FILTER_SIZES = [2, 3, 4]\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "EPOCHS = 20 # Increased epochs slightly, as early stopping might trigger\n",
    "PATIENCE = 3 # Early stopping patience: stop after 3 epochs with no improvement\n",
    "DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 2. Data Loading and Initial Cleaning ---\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "    print(\"File loaded successfully.\")\n",
    "    required_cols = [TEXT_COLUMN, SENTIMENT_COLUMN, ASPECT_COLUMN]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Required columns {missing} not found in the file.\")\n",
    "    df = df.dropna(subset=required_cols)\n",
    "    df = df[required_cols].copy()\n",
    "    df = df[df[TEXT_COLUMN].str.strip().astype(bool)]\n",
    "    print(f\"Data shape after loading and initial cleaning: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {FILE_PATH}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Text Preprocessing and Vocabulary Building ---\n",
    "def preprocess_text(text):\n",
    "    return str(text).lower()\n",
    "\n",
    "def tokenize_vi(text):\n",
    "    try:\n",
    "        return word_tokenize(str(text), format=\"text\").split()\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "df[TEXT_COLUMN] = df[TEXT_COLUMN].apply(preprocess_text)\n",
    "df['tokens'] = df[TEXT_COLUMN].apply(tokenize_vi)\n",
    "\n",
    "word_counts = Counter(word for tokens in df['tokens'] for word in tokens)\n",
    "vocab = [word for word, count in word_counts.most_common(VOCAB_SIZE - 2)]\n",
    "word_to_ix = {word: i+2 for i, word in enumerate(vocab)}\n",
    "word_to_ix['<PAD>'] = 0\n",
    "word_to_ix['<UNK>'] = 1\n",
    "actual_vocab_size = len(word_to_ix)\n",
    "print(f\"Actual vocabulary size: {actual_vocab_size}\")\n",
    "\n",
    "# --- 4. Data Encoding ---\n",
    "def tokens_to_indices(tokens, word_to_ix, max_len):\n",
    "    indices = [word_to_ix.get(token, word_to_ix['<UNK>']) for token in tokens]\n",
    "    indices = indices[:max_len]\n",
    "    return indices + [word_to_ix['<PAD>']] * (max_len - len(indices))\n",
    "\n",
    "df['indices'] = df['tokens'].apply(lambda x: tokens_to_indices(x, word_to_ix, MAX_LEN))\n",
    "\n",
    "unique_sentiments = sorted(df[SENTIMENT_COLUMN].unique()) # Sort for consistent mapping\n",
    "sentiment_to_ix = {label: i for i, label in enumerate(unique_sentiments)}\n",
    "ix_to_sentiment = {i: label for label, i in sentiment_to_ix.items()}\n",
    "num_sentiment_classes = len(unique_sentiments)\n",
    "print(f\"Sentiment Labels: {sentiment_to_ix}\")\n",
    "\n",
    "unique_aspects = sorted(df[ASPECT_COLUMN].unique()) # Sort for consistent mapping\n",
    "aspect_to_ix = {label: i for i, label in enumerate(unique_aspects)}\n",
    "ix_to_aspect = {i: label for label, i in aspect_to_ix.items()}\n",
    "num_aspect_classes = len(unique_aspects)\n",
    "print(f\"Aspect Labels: {aspect_to_ix}\")\n",
    "\n",
    "df['sentiment_encoded'] = df[SENTIMENT_COLUMN].map(sentiment_to_ix)\n",
    "df['aspect_encoded'] = df[ASPECT_COLUMN].map(aspect_to_ix)\n",
    "\n",
    "# --- 5. Data Splitting (Train/Validation/Test) ---\n",
    "X = list(df['indices'])\n",
    "y_sentiment = list(df['sentiment_encoded'])\n",
    "y_aspect = list(df['aspect_encoded'])\n",
    "\n",
    "# Combine labels for stratification during the first split\n",
    "y_combined_for_stratify = [f\"{s}_{a}\" for s, a in zip(y_sentiment, y_aspect)]\n",
    "\n",
    "# First split: Train+Validation (80%) vs Test (20%)\n",
    "X_temp, X_test, y_sentiment_temp, y_sentiment_test, y_aspect_temp, y_aspect_test = train_test_split(\n",
    "    X, y_sentiment, y_aspect, test_size=0.20, random_state=42, stratify=y_combined_for_stratify\n",
    ")\n",
    "\n",
    "# Combine labels again for stratification during the second split\n",
    "y_combined_temp_for_stratify = [f\"{s}_{a}\" for s, a in zip(y_sentiment_temp, y_aspect_temp)]\n",
    "\n",
    "# Second split: Train (80% of temp -> 64% of total) vs Validation (20% of temp -> 16% of total)\n",
    "# test_size = 0.20 means 20% of the temp set (which is 80% of total) goes to validation -> 0.20 * 0.80 = 0.16\n",
    "X_train, X_val, y_sentiment_train, y_sentiment_val, y_aspect_train, y_aspect_val = train_test_split(\n",
    "    X_temp, y_sentiment_temp, y_aspect_temp, test_size=0.20, random_state=42, stratify=y_combined_temp_for_stratify\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "# --- 6. Dataset and DataLoader Creation ---\n",
    "class AspectSentimentDataset(Dataset):\n",
    "    def __init__(self, indices, sentiment_labels, aspect_labels):\n",
    "        self.indices = indices\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.aspect_labels = aspect_labels\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.indices[idx], dtype=torch.long),\n",
    "                torch.tensor(self.sentiment_labels[idx], dtype=torch.long),\n",
    "                torch.tensor(self.aspect_labels[idx], dtype=torch.long))\n",
    "\n",
    "train_dataset = AspectSentimentDataset(X_train, y_sentiment_train, y_aspect_train)\n",
    "val_dataset = AspectSentimentDataset(X_val, y_sentiment_val, y_aspect_val)\n",
    "test_dataset = AspectSentimentDataset(X_test, y_sentiment_test, y_aspect_test) # Test Dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # Test DataLoader\n",
    "\n",
    "# --- 7. Model Definition (CNN_LSTM_MTL) ---\n",
    "# (Model definition remains the same as before)\n",
    "class CNN_LSTM_MTL(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes,\n",
    "                 lstm_hidden_dim, num_sentiment_classes, num_aspect_classes,\n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters,\n",
    "                      kernel_size=fs, padding=(fs - 1) // 2)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        lstm_input_size = num_filters\n",
    "        self.lstm = nn.LSTM(lstm_input_size, lstm_hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=bidirectional, batch_first=True,\n",
    "                            dropout=dropout if n_layers > 1 else 0)\n",
    "        fc_input_dim = lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim\n",
    "        self.fc_sentiment = nn.Linear(fc_input_dim, num_sentiment_classes)\n",
    "        self.fc_aspect = nn.Linear(fc_input_dim, num_aspect_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        # Using only the first conv layer's output for simplicity\n",
    "        conved = torch.relu(self.convs[0](embedded))\n",
    "        conved = conved.permute(0, 2, 1)\n",
    "        lstm_output, (hidden, cell) = self.lstm(conved)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=-1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        output_sentiment = self.fc_sentiment(hidden)\n",
    "        output_aspect = self.fc_aspect(hidden)\n",
    "        return output_sentiment, output_aspect\n",
    "\n",
    "# --- 8. Initialization ---\n",
    "PAD_IDX = word_to_ix['<PAD>']\n",
    "model = CNN_LSTM_MTL(\n",
    "    vocab_size=actual_vocab_size, embedding_dim=EMBEDDING_DIM, num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES, lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
    "    num_sentiment_classes=num_sentiment_classes, num_aspect_classes=num_aspect_classes,\n",
    "    n_layers=N_LAYERS, bidirectional=BIDIRECTIONAL, dropout=DROPOUT, pad_idx=PAD_IDX\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# --- 9. Training Setup ---\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion_sentiment = nn.CrossEntropyLoss().to(DEVICE)\n",
    "criterion_aspect = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "def calculate_accuracy(preds, y):\n",
    "    top_pred = preds.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    return correct.float() / y.shape[0]\n",
    "\n",
    "# --- 10. Training and Evaluation Loops (Modified for Early Stopping) ---\n",
    "# (train_epoch remains the same as before)\n",
    "def train_epoch(model, iterator, optimizer, criterion_sentiment, criterion_aspect):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc_sentiment = 0\n",
    "    epoch_acc_aspect = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        text, sentiment_labels, aspect_labels = batch\n",
    "        text, sentiment_labels, aspect_labels = text.to(DEVICE), sentiment_labels.to(DEVICE), aspect_labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        predictions_sentiment, predictions_aspect = model(text)\n",
    "        loss_sentiment = criterion_sentiment(predictions_sentiment, sentiment_labels)\n",
    "        loss_aspect = criterion_aspect(predictions_aspect, aspect_labels)\n",
    "        total_loss = loss_sentiment + loss_aspect\n",
    "        acc_sentiment = calculate_accuracy(predictions_sentiment, sentiment_labels)\n",
    "        acc_aspect = calculate_accuracy(predictions_aspect, aspect_labels)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_acc_sentiment += acc_sentiment.item()\n",
    "        epoch_acc_aspect += acc_aspect.item()\n",
    "    return (epoch_loss / len(iterator), epoch_acc_sentiment / len(iterator), epoch_acc_aspect / len(iterator))\n",
    "\n",
    "# (evaluate_epoch remains the same as before - used for validation)\n",
    "def evaluate_epoch(model, iterator, criterion_sentiment, criterion_aspect):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc_sentiment = 0\n",
    "    epoch_acc_aspect = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, sentiment_labels, aspect_labels = batch\n",
    "            text, sentiment_labels, aspect_labels = text.to(DEVICE), sentiment_labels.to(DEVICE), aspect_labels.to(DEVICE)\n",
    "            predictions_sentiment, predictions_aspect = model(text)\n",
    "            loss_sentiment = criterion_sentiment(predictions_sentiment, sentiment_labels)\n",
    "            loss_aspect = criterion_aspect(predictions_aspect, aspect_labels)\n",
    "            total_loss = loss_sentiment + loss_aspect\n",
    "            acc_sentiment = calculate_accuracy(predictions_sentiment, sentiment_labels)\n",
    "            acc_aspect = calculate_accuracy(predictions_aspect, aspect_labels)\n",
    "            epoch_loss += total_loss.item()\n",
    "            epoch_acc_sentiment += acc_sentiment.item()\n",
    "            epoch_acc_aspect += acc_aspect.item()\n",
    "    return (epoch_loss / len(iterator), epoch_acc_sentiment / len(iterator), epoch_acc_aspect / len(iterator))\n",
    "\n",
    "# --- 11. Main Training Loop with Early Stopping ---\n",
    "best_valid_loss = float('inf')\n",
    "epochs_no_improve = 0 # Counter for early stopping\n",
    "saved_model_path = 'cnn_lstm_mtl_best_model.pt' # Path to save best model\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc_sent, train_acc_aspect = train_epoch(\n",
    "        model, train_loader, optimizer, criterion_sentiment, criterion_aspect\n",
    "    )\n",
    "    valid_loss, valid_acc_sent, valid_acc_aspect = evaluate_epoch(\n",
    "        model, val_loader, criterion_sentiment, criterion_aspect\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc Sent: {train_acc_sent*100:.2f}% | Train Acc Aspect: {train_acc_aspect*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc Sent: {valid_acc_sent*100:.2f}% |  Val. Acc Aspect: {valid_acc_aspect*100:.2f}%')\n",
    "\n",
    "    # --- Early Stopping Logic ---\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), saved_model_path)\n",
    "        epochs_no_improve = 0 # Reset counter\n",
    "        print(f'\\tValidation loss decreased ({best_valid_loss:.3f}). Saving model...')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f'\\tValidation loss did not decrease. ({epochs_no_improve}/{PATIENCE})')\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f'\\nEarly stopping triggered after {epoch + 1} epochs.')\n",
    "            break # Exit training loop\n",
    "\n",
    "if epochs_no_improve < PATIENCE:\n",
    "    print(\"\\nTraining finished after completing all epochs.\")\n",
    "\n",
    "# --- 12. Test Set Evaluation ---\n",
    "print(f\"\\nLoading best model from '{saved_model_path}' for test evaluation...\")\n",
    "try:\n",
    "    model.load_state_dict(torch.load(saved_model_path, map_location=DEVICE))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Best model file '{saved_model_path}' not found. Evaluation cannot proceed.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "     print(f\"Error loading model weights: {e}. Evaluation cannot proceed.\")\n",
    "     exit()\n",
    "\n",
    "\n",
    "def evaluate_on_test_set(model, iterator, criterion_sentiment, criterion_aspect):\n",
    "    \"\"\"Evaluates the model on the test set and collects predictions.\"\"\"\n",
    "    test_loss = 0\n",
    "    test_acc_sentiment = 0\n",
    "    test_acc_aspect = 0\n",
    "\n",
    "    all_predictions_sentiment = []\n",
    "    all_predictions_aspect = []\n",
    "    all_true_sentiment = []\n",
    "    all_true_aspect = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, sentiment_labels, aspect_labels = batch\n",
    "            text, sentiment_labels, aspect_labels = text.to(DEVICE), sentiment_labels.to(DEVICE), aspect_labels.to(DEVICE)\n",
    "\n",
    "            predictions_sentiment, predictions_aspect = model(text)\n",
    "\n",
    "            loss_sentiment = criterion_sentiment(predictions_sentiment, sentiment_labels)\n",
    "            loss_aspect = criterion_aspect(predictions_aspect, aspect_labels)\n",
    "            total_loss = loss_sentiment + loss_aspect\n",
    "\n",
    "            acc_sentiment = calculate_accuracy(predictions_sentiment, sentiment_labels)\n",
    "            acc_aspect = calculate_accuracy(predictions_aspect, aspect_labels)\n",
    "\n",
    "            test_loss += total_loss.item()\n",
    "            test_acc_sentiment += acc_sentiment.item()\n",
    "            test_acc_aspect += acc_aspect.item()\n",
    "\n",
    "            # Store predictions and true labels for detailed metrics\n",
    "            all_predictions_sentiment.extend(predictions_sentiment.argmax(1).cpu().numpy())\n",
    "            all_predictions_aspect.extend(predictions_aspect.argmax(1).cpu().numpy())\n",
    "            all_true_sentiment.extend(sentiment_labels.cpu().numpy())\n",
    "            all_true_aspect.extend(aspect_labels.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(iterator)\n",
    "    avg_test_acc_sentiment = test_acc_sentiment / len(iterator)\n",
    "    avg_test_acc_aspect = test_acc_aspect / len(iterator)\n",
    "\n",
    "    return (avg_test_loss, avg_test_acc_sentiment, avg_test_acc_aspect,\n",
    "            all_true_sentiment, all_predictions_sentiment,\n",
    "            all_true_aspect, all_predictions_aspect)\n",
    "\n",
    "# Run evaluation on the test set\n",
    "(test_loss, test_acc_sent, test_acc_aspect,\n",
    " true_sent, pred_sent,\n",
    " true_aspect, pred_aspect) = evaluate_on_test_set(model, test_loader, criterion_sentiment, criterion_aspect)\n",
    "\n",
    "print(f'\\n--- Test Set Evaluation Results ---')\n",
    "print(f'Test Loss: {test_loss:.3f}')\n",
    "print(f'Test Sentiment Accuracy: {test_acc_sent*100:.2f}%')\n",
    "print(f'Test Aspect Accuracy: {test_acc_aspect*100:.2f}%')\n",
    "\n",
    "# --- 13. Detailed Test Metrics (Classification Report & Confusion Matrix) ---\n",
    "\n",
    "# Get label names from the mappings (ensure they are sorted correctly)\n",
    "sentiment_label_names = [ix_to_sentiment[i] for i in range(num_sentiment_classes)]\n",
    "aspect_label_names = [ix_to_aspect[i] for i in range(num_aspect_classes)]\n",
    "\n",
    "print(\"\\n--- Sentiment Classification Report (Test Set) ---\")\n",
    "print(classification_report(true_sent, pred_sent, target_names=sentiment_label_names, zero_division=0))\n",
    "\n",
    "print(\"\\n--- Sentiment Confusion Matrix (Test Set) ---\")\n",
    "cm_sentiment = confusion_matrix(true_sent, pred_sent)\n",
    "print(\"Labels:\", sentiment_label_names)\n",
    "print(cm_sentiment)\n",
    "\n",
    "print(\"\\n--- Aspect Classification Report (Test Set) ---\")\n",
    "print(classification_report(true_aspect, pred_aspect, target_names=aspect_label_names, zero_division=0))\n",
    "\n",
    "print(\"\\n--- Aspect Confusion Matrix (Test Set) ---\")\n",
    "cm_aspect = confusion_matrix(true_aspect, pred_aspect)\n",
    "# Try to make aspect CM labels clearer if many classes\n",
    "if len(aspect_label_names) <= 10:\n",
    "    print(\"Labels:\", aspect_label_names)\n",
    "    print(cm_aspect)\n",
    "else:\n",
    "     print(\"(Aspect labels omitted due to large number)\")\n",
    "     print(cm_aspect)\n",
    "\n",
    "\n",
    "# --- 14. Example Predictions (Using the best loaded model) ---\n",
    "# (Prediction function remains the same as before)\n",
    "def predict_aspect_sentiment(sentence, model, word_to_ix, max_len, device, ix_to_sentiment, ix_to_aspect):\n",
    "    model.eval()\n",
    "    processed_text = preprocess_text(sentence)\n",
    "    tokens = tokenize_vi(processed_text)\n",
    "    indexed = tokens_to_indices(tokens, word_to_ix, max_len)\n",
    "    tensor = torch.LongTensor(indexed).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction_sentiment, prediction_aspect = model(tensor)\n",
    "    probs_sentiment = torch.softmax(prediction_sentiment, dim=1)\n",
    "    probs_aspect = torch.softmax(prediction_aspect, dim=1)\n",
    "    predicted_class_sentiment = prediction_sentiment.argmax(dim=1).item()\n",
    "    predicted_label_sentiment = ix_to_sentiment[predicted_class_sentiment]\n",
    "    confidence_sentiment = probs_sentiment.max().item()\n",
    "    predicted_class_aspect = prediction_aspect.argmax(dim=1).item()\n",
    "    predicted_label_aspect = ix_to_aspect[predicted_class_aspect]\n",
    "    confidence_aspect = probs_aspect.max().item()\n",
    "    return predicted_label_sentiment, confidence_sentiment, predicted_label_aspect, confidence_aspect\n",
    "\n",
    "print(\"\\n--- Example Predictions (using best model) ---\")\n",
    "test_sentences = [\n",
    "    \"Giảng viên dạy rất hay và nhiệt tình.\",\n",
    "    \"Nội dung môn học quá cũ, không cập nhật.\",\n",
    "    \"Tài liệu học tập đầy đủ.\",\n",
    "    \"Phòng học quá nóng và thiếu ánh sáng.\",\n",
    "    \"Thầy cô hỗ trợ sinh viên rất tốt ngoài giờ học.\"\n",
    "]\n",
    "for sentence in test_sentences:\n",
    "    pred_sent, conf_sent, pred_aspect, conf_aspect = predict_aspect_sentiment(\n",
    "        sentence, model, word_to_ix, MAX_LEN, DEVICE, ix_to_sentiment, ix_to_aspect\n",
    "    )\n",
    "    print(f'\\nSentence:  \"{sentence}\"')\n",
    "    print(f'Predicted: Aspect = {pred_aspect} (Conf: {conf_aspect:.4f}), Sentiment = {pred_sent} (Conf: {conf_sent:.4f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
